{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from src.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from src.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"tokenizers/cnn_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a233301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247315/1764970837.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_highlight_first_train.pkl\"))\n",
      "/tmp/ipykernel_247315/1764970837.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_highlight_last_train.pkl\"))\n"
     ]
    }
   ],
   "source": [
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_highlight_first_train.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_highlight_last_train.pkl\"))\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2), dim=0)\n",
    "\n",
    "corpus_test1 = torch.tensor(loader(\"corpus/cnn_dailymail_highlight_first_test.pkl\"))\n",
    "corpus_test2 = torch.tensor(loader(\"corpus/cnn_dailymail_highlight_last_test.pkl\"))\n",
    "corpus_test = torch.cat((corpus_test1, corpus_test2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 315778058\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=tokenizer.token_to_idx[\"<s>\"],\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "print(\"Number of parameters:\", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ee96f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_checkpoint(model, \n",
    "                optimizer, \n",
    "                scheduler,\n",
    "                loss_train_list,\n",
    "                loss_test_list, \n",
    "                filename=\"models/checkpoint_transformer.pth\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7c59",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[model, \n",
    "optimizer, \n",
    "scheduler, \n",
    "loss_train_list, \n",
    "loss_test_list] = load_checkpoint(\"models/checkpoint_transformer.pth\", \n",
    "                                  model, \n",
    "                                  optimizer, \n",
    "                                  scheduler, \n",
    "                                  loss_train_list, \n",
    "                                  loss_test_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7e88a2e9ef48fb88fa614c0ed5355f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 3.0101, Loss_eval: 3.3781, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 3.1362, Loss_eval: 3.3859, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 3.1878, Loss_eval: 3.3847, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 3.1127, Loss_eval: 3.3824, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 2.9768, Loss_eval: 3.3939, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 2.9542, Loss_eval: 3.3924, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 2.8370, Loss_eval: 3.3782, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 3.2016, Loss_eval: 3.3700, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 3.1186, Loss_eval: 3.3971, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 3.2227, Loss_eval: 3.3826, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 3.1356, Loss_eval: 3.3757, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 3.2055, Loss_eval: 3.3679, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 3.1512, Loss_eval: 3.3647, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 3.0872, Loss_eval: 3.3687, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.0213, Loss_eval: 3.3655, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 3.4208, Loss_eval: 3.3585, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 3.0904, Loss_eval: 3.3947, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 3.1433, Loss_eval: 3.3676, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 3.1708, Loss_eval: 3.3915, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 3.1252, Loss_eval: 3.3889, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 3.1743, Loss_eval: 3.3777, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 3.1506, Loss_eval: 3.3681, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.1294, Loss_eval: 3.3846, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.0324, Loss_eval: 3.3734, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 3.2419, Loss_eval: 3.3804, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 3.1448, Loss_eval: 3.3900, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.0674, Loss_eval: 3.3703, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 2.9345, Loss_eval: 3.3742, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 3.4166, Loss_eval: 3.3612, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 3.3796, Loss_eval: 3.3872, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 3.1520, Loss_eval: 3.3778, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.0890, Loss_eval: 3.3633, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.1631, Loss_eval: 3.3615, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 2.9294, Loss_eval: 3.3771, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 2.9577, Loss_eval: 3.3955, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.1182, Loss_eval: 3.3947, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 3.0852, Loss_eval: 3.3887, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 3.2185, Loss_eval: 3.3817, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 3.3984, Loss_eval: 3.3700, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 3.1851, Loss_eval: 3.3772, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 2.9385, Loss_eval: 3.3756, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 2.8839, Loss_eval: 3.3666, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 3.1227, Loss_eval: 3.3827, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.0813, Loss_eval: 3.3929, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 3.2972, Loss_eval: 3.3736, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 2.8865, Loss_eval: 3.3738, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.1322, Loss_eval: 3.3772, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 3.1407, Loss_eval: 3.3844, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 3.2277, Loss_eval: 3.3859, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 3.2704, Loss_eval: 3.3906, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 2.9758, Loss_eval: 3.3696, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.0928, Loss_eval: 3.3748, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 3.2368, Loss_eval: 3.3863, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 2.9196, Loss_eval: 3.3523, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 3.3380, Loss_eval: 3.3613, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.0579, Loss_eval: 3.3744, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 3.1528, Loss_eval: 3.3775, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 3.0768, Loss_eval: 3.3724, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.1375, Loss_eval: 3.3696, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 3.2809, Loss_eval: 3.3854, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.0019, Loss_eval: 3.3936, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 3.3426, Loss_eval: 3.3920, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.1873, Loss_eval: 3.3598, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 3.3191, Loss_eval: 3.3624, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.0040, Loss_eval: 3.3910, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 2.8676, Loss_eval: 3.3739, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.2047, Loss_eval: 3.3648, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.0194, Loss_eval: 3.3884, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.1509, Loss_eval: 3.3800, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.1807, Loss_eval: 3.3957, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 3.2664, Loss_eval: 3.3677, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 2.8289, Loss_eval: 3.3671, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.1127, Loss_eval: 3.3644, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.0232, Loss_eval: 3.3930, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 3.2298, Loss_eval: 3.3611, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.0104, Loss_eval: 3.3865, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 3.5132, Loss_eval: 3.3750, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.0643, Loss_eval: 3.3813, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 3.4205, Loss_eval: 3.3654, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 3.2645, Loss_eval: 3.3690, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.1132, Loss_eval: 3.3653, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 2.9905, Loss_eval: 3.3707, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 2.9603, Loss_eval: 3.3617, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.3260, Loss_eval: 3.3695, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.1546, Loss_eval: 3.3690, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.0780, Loss_eval: 3.3644, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 3.2225, Loss_eval: 3.3839, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.0204, Loss_eval: 3.3728, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.1646, Loss_eval: 3.3752, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.0503, Loss_eval: 3.3779, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 3.0024, Loss_eval: 3.3972, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 3.0578, Loss_eval: 3.3794, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.2174, Loss_eval: 3.3816, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 3.2390, Loss_eval: 3.3860, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 2.7878, Loss_eval: 3.3631, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.0686, Loss_eval: 3.3607, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 2.6325, Loss_eval: 3.3402, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 3.3377, Loss_eval: 3.3818, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 3.0841, Loss_eval: 3.3663, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.0591, Loss_eval: 3.3770, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 3.2517, Loss_eval: 3.3594, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.0779, Loss_eval: 3.3588, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 3.1967, Loss_eval: 3.3835, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.0487, Loss_eval: 3.3926, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 3.3278, Loss_eval: 3.3791, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.2822, Loss_eval: 3.3701, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.1151, Loss_eval: 3.3737, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 2.9537, Loss_eval: 3.3701, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 3.2521, Loss_eval: 3.3720, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 3.2586, Loss_eval: 3.3773, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.2016, Loss_eval: 3.3829, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 2.8381, Loss_eval: 3.3422, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 3.5896, Loss_eval: 3.3432, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.0503, Loss_eval: 3.3778, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 3.1872, Loss_eval: 3.3647, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.0441, Loss_eval: 3.3472, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 3.1112, Loss_eval: 3.3638, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 3.2265, Loss_eval: 3.3708, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 2.9124, Loss_eval: 3.3831, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 3.0595, Loss_eval: 3.3611, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 3.0692, Loss_eval: 3.3897, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 2.7891, Loss_eval: 3.3725, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 2.9523, Loss_eval: 3.3772, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.1028, Loss_eval: 3.3802, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 2.9917, Loss_eval: 3.3685, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.1986, Loss_eval: 3.3721, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 3.2838, Loss_eval: 3.3838, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 2.8898, Loss_eval: 3.3717, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 2.9867, Loss_eval: 3.3722, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 2.8683, Loss_eval: 3.3540, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 2.7896, Loss_eval: 3.3883, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 3.0316, Loss_eval: 3.3695, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 2.8160, Loss_eval: 3.3526, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 3.4225, Loss_eval: 3.3703, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 2.8108, Loss_eval: 3.3522, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.1078, Loss_eval: 3.3814, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 2.9881, Loss_eval: 3.3839, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.0748, Loss_eval: 3.3590, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 2.9298, Loss_eval: 3.3753, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 2.9122, Loss_eval: 3.3582, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.1229, Loss_eval: 3.3558, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.1805, Loss_eval: 3.3472, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 2.7974, Loss_eval: 3.3607, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 2.9853, Loss_eval: 3.3688, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.0135, Loss_eval: 3.3544, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.2598, Loss_eval: 3.3342, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.0461, Loss_eval: 3.3460, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.1340, Loss_eval: 3.3734, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.1769, Loss_eval: 3.3584, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 2.9201, Loss_eval: 3.3861, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 2.9096, Loss_eval: 3.3670, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 2.7328, Loss_eval: 3.3770, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 2.9544, Loss_eval: 3.3656, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 3.1726, Loss_eval: 3.3608, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 2.8407, Loss_eval: 3.3835, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 3.1898, Loss_eval: 3.3746, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 2.9763, Loss_eval: 3.3661, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 2.8860, Loss_eval: 3.3503, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 2.9551, Loss_eval: 3.3798, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 3.3844, Loss_eval: 3.3780, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 3.2113, Loss_eval: 3.3780, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 2.7964, Loss_eval: 3.3761, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 3.4639, Loss_eval: 3.3704, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 3.1731, Loss_eval: 3.3735, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 2.8634, Loss_eval: 3.3706, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 3.3045, Loss_eval: 3.3695, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 3.0213, Loss_eval: 3.3548, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 2.9239, Loss_eval: 3.3798, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 2.9495, Loss_eval: 3.3694, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 3.2557, Loss_eval: 3.3469, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 3.2834, Loss_eval: 3.3695, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 3.0608, Loss_eval: 3.3735, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 3.2940, Loss_eval: 3.3510, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 2.9996, Loss_eval: 3.3402, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 2.7320, Loss_eval: 3.3845, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 2.8553, Loss_eval: 3.3757, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 3.0851, Loss_eval: 3.3518, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 3.0207, Loss_eval: 3.3737, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 3.1095, Loss_eval: 3.3427, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 3.0028, Loss_eval: 3.3791, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 3.1185, Loss_eval: 3.3870, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 2.9664, Loss_eval: 3.3494, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 3.0850, Loss_eval: 3.3676, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 3.1109, Loss_eval: 3.3500, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 3.4892, Loss_eval: 3.3872, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 3.2318, Loss_eval: 3.3682, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 3.3364, Loss_eval: 3.3657, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 2.9481, Loss_eval: 3.3756, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 3.2213, Loss_eval: 3.3684, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 3.0915, Loss_eval: 3.3891, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 2.9839, Loss_eval: 3.3965, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 2.9157, Loss_eval: 3.3605, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 3.1260, Loss_eval: 3.3755, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 3.0009, Loss_eval: 3.3726, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 3.2254, Loss_eval: 3.3605, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 2.7091, Loss_eval: 3.3765, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 2.9696, Loss_eval: 3.3429, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 2.8024, Loss_eval: 3.3522, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 3.1883, Loss_eval: 3.3595, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 2.8399, Loss_eval: 3.3656, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 3.0525, Loss_eval: 3.3663, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 3.1499, Loss_eval: 3.3401, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 2.9301, Loss_eval: 3.3599, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 3.0945, Loss_eval: 3.3855, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 3.3057, Loss_eval: 3.3807, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 3.2465, Loss_eval: 3.3735, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 3.1328, Loss_eval: 3.3803, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 3.0175, Loss_eval: 3.3715, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 3.2830, Loss_eval: 3.3439, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 2.9285, Loss_eval: 3.3662, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 2.8569, Loss_eval: 3.3601, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 3.0970, Loss_eval: 3.3551, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 2.7863, Loss_eval: 3.3404, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 2.9468, Loss_eval: 3.3698, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 3.2890, Loss_eval: 3.3493, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 2.7298, Loss_eval: 3.3705, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 2.8066, Loss_eval: 3.3680, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 2.7269, Loss_eval: 3.3722, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 3.0104, Loss_eval: 3.3784, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 3.1919, Loss_eval: 3.3640, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 3.0503, Loss_eval: 3.3625, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 3.4012, Loss_eval: 3.3596, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 2.8673, Loss_eval: 3.3654, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 2.9841, Loss_eval: 3.3652, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 3.1055, Loss_eval: 3.3503, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 3.1358, Loss_eval: 3.3733, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 3.1076, Loss_eval: 3.3500, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 3.1639, Loss_eval: 3.3512, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 3.7132, Loss_eval: 3.3486, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 3.0707, Loss_eval: 3.3884, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 3.1845, Loss_eval: 3.3546, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 3.2834, Loss_eval: 3.3558, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 3.0195, Loss_eval: 3.3769, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 3.1723, Loss_eval: 3.3593, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 3.2092, Loss_eval: 3.3884, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 3.0074, Loss_eval: 3.3863, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 3.3957, Loss_eval: 3.3614, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 3.2469, Loss_eval: 3.3650, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 3.1279, Loss_eval: 3.3481, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 2.9164, Loss_eval: 3.3683, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 3.2622, Loss_eval: 3.3694, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 3.0371, Loss_eval: 3.3843, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 3.1751, Loss_eval: 3.3638, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 3.1874, Loss_eval: 3.3522, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 3.2081, Loss_eval: 3.3600, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 2.9465, Loss_eval: 3.3670, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 3.0379, Loss_eval: 3.3588, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 3.4451, Loss_eval: 3.3761, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 2.8729, Loss_eval: 3.3755, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 3.0341, Loss_eval: 3.3528, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 2.9505, Loss_eval: 3.3518, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 3.2812, Loss_eval: 3.3516, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 2.9723, Loss_eval: 3.3578, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 2.9408, Loss_eval: 3.3483, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 2.7767, Loss_eval: 3.3729, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 2.8670, Loss_eval: 3.3636, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 3.0572, Loss_eval: 3.3530, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 3.0528, Loss_eval: 3.3412, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 3.1392, Loss_eval: 3.3503, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 3.3899, Loss_eval: 3.3320, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 2.9117, Loss_eval: 3.3394, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 2.9744, Loss_eval: 3.3454, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 2.9975, Loss_eval: 3.3568, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 3.2361, Loss_eval: 3.3570, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 3.1405, Loss_eval: 3.3601, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 2.6897, Loss_eval: 3.3475, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 3.4345, Loss_eval: 3.3553, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 3.0565, Loss_eval: 3.3659, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 3.2701, Loss_eval: 3.3434, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 3.1677, Loss_eval: 3.3647, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 3.0477, Loss_eval: 3.3492, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 2.9786, Loss_eval: 3.3693, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 3.4026, Loss_eval: 3.3343, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 3.2198, Loss_eval: 3.3395, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 3.0395, Loss_eval: 3.3443, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 2.8400, Loss_eval: 3.3410, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 3.3485, Loss_eval: 3.3518, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 3.0144, Loss_eval: 3.3637, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 3.3977, Loss_eval: 3.3618, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 3.1701, Loss_eval: 3.3574, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 3.0433, Loss_eval: 3.3462, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 3.0547, Loss_eval: 3.3686, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 3.2233, Loss_eval: 3.3538, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 3.2648, Loss_eval: 3.3519, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 3.0001, Loss_eval: 3.3600, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 2.9270, Loss_eval: 3.3443, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 3.1250, Loss_eval: 3.3302, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 3.2210, Loss_eval: 3.3678, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 2.8613, Loss_eval: 3.3598, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 3.0193, Loss_eval: 3.3703, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 3.2095, Loss_eval: 3.3537, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 3.3661, Loss_eval: 3.3243, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 3.0503, Loss_eval: 3.3567, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 2.9409, Loss_eval: 3.3535, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 3.0668, Loss_eval: 3.3535, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 3.3036, Loss_eval: 3.3526, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 3.1233, Loss_eval: 3.3417, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 3.3540, Loss_eval: 3.3628, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 2.8887, Loss_eval: 3.3416, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 3.0752, Loss_eval: 3.3527, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 2.8767, Loss_eval: 3.3535, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 3.5271, Loss_eval: 3.3490, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 3.2680, Loss_eval: 3.3506, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 2.9973, Loss_eval: 3.3565, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 2.9755, Loss_eval: 3.3635, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 3.4253, Loss_eval: 3.3394, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 3.0214, Loss_eval: 3.3541, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 3.0827, Loss_eval: 3.3562, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 3.0844, Loss_eval: 3.3486, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 3.3425, Loss_eval: 3.3574, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.0394, Loss_eval: 3.3397, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 3.2787, Loss_eval: 3.3580, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.0335, Loss_eval: 3.3518, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 3.3707, Loss_eval: 3.3647, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 3.2460, Loss_eval: 3.3564, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 2.8888, Loss_eval: 3.3448, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 2.8472, Loss_eval: 3.3345, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 3.0116, Loss_eval: 3.3445, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 3.2190, Loss_eval: 3.3395, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 3.0508, Loss_eval: 3.3637, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 2.8877, Loss_eval: 3.3561, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.0468, Loss_eval: 3.3298, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.0188, Loss_eval: 3.3674, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 3.0586, Loss_eval: 3.3286, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 3.2079, Loss_eval: 3.3570, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 3.0706, Loss_eval: 3.3564, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 3.1518, Loss_eval: 3.3524, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 2.9082, Loss_eval: 3.3325, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 2.9856, Loss_eval: 3.3802, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 3.0582, Loss_eval: 3.3649, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 2.8764, Loss_eval: 3.3736, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 2.9709, Loss_eval: 3.3387, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 2.9885, Loss_eval: 3.3523, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 2.9891, Loss_eval: 3.3493, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 3.4203, Loss_eval: 3.3486, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 3.2361, Loss_eval: 3.3680, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 3.1671, Loss_eval: 3.3715, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 3.1165, Loss_eval: 3.3475, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 3.2202, Loss_eval: 3.3445, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 2.8870, Loss_eval: 3.3462, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 2.8663, Loss_eval: 3.3384, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 3.3103, Loss_eval: 3.3647, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 2.8546, Loss_eval: 3.3465, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 3.0289, Loss_eval: 3.3298, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 3.3501, Loss_eval: 3.3334, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.0556, Loss_eval: 3.3660, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 3.1077, Loss_eval: 3.3455, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 2.8312, Loss_eval: 3.3680, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 3.2195, Loss_eval: 3.3475, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 2.9782, Loss_eval: 3.3424, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=\"checkpoint_transformer.pth\")\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    loss_train_list,\n",
    "                            loss_test_list,  \n",
    "                    filename=\"checkpoint_transformer.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f2ca710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train_list)\n",
    "plt.plot(loss_test_list)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Evaluation Loss\")\n",
    "plt.legend([\"Training Loss\", \"Evaluation Loss\"])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
