{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from src.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from src.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"tokenizers/cnn_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=tokenizer.token_to_idx[\"<s>\"],\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "loss_train_list = []\n",
    "loss_eval_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9566797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = 1\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b97d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = load_checkpoint(\"models/checkpoint_transformer.pth\", model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fc4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model, tokenizer, context_length, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def run(self, text, T, k, mode=None):\n",
    "        if mode == \"summary\":\n",
    "            text = \"<s><b>\" + text + \"<h>\"\n",
    "        elif mode == \"expand\":\n",
    "            text = \"<s><h>\" + text + \"<b>\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        tokens = torch.tensor(self.tokenizer.encode(text.lower()), dtype=torch.long).reshape(1, -1).to(self.device)\n",
    "\n",
    "        self.display = Display()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.context_length):\n",
    "                next = self.next_token(tokens, T, k,)\n",
    "\n",
    "                tokens = torch.cat([tokens, next.reshape(1,1)], dim=1)\n",
    "                text = tokenizer.decode(tokens[0].tolist())\n",
    "                self.display.update(text)\n",
    "\n",
    "                if next[0] == tokenizer.token_to_idx[\"</s>\"]:\n",
    "                    break\n",
    "                \n",
    "\n",
    "    def next_token(self, tokens, T, k):\n",
    "        logits = self.model(tokens)[0, -1:]\n",
    "        topk_vals, _    = torch.topk(logits, k=k)\n",
    "        kth_value       = topk_vals[:,-1]\n",
    "\n",
    "        logits = torch.where(logits >= kth_value, logits, -torch.inf)\n",
    "        dist = Categorical(logits=logits/T)\n",
    "        next = dist.sample()\n",
    "\n",
    "        return next\n",
    "\n",
    "\n",
    "class Display:\n",
    "    def __init__(self):\n",
    "        self.wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "        self.ta = widgets.Textarea(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(width='80ch', height='20em'),\n",
    "            disabled=True\n",
    "        )\n",
    "        display(self.ta)\n",
    "\n",
    "    def update(self, text):\n",
    "        self.ta.value = self.wrapper.fill(text.replace(\"\\n\", \" \"))  # this updates in-place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4c7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = Inference(model, tokenizer, context_length=1024, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962b0b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08314412473649a6a416adbd12902fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m T = \u001b[32m1\u001b[39m\n\u001b[32m      4\u001b[39m k = \u001b[32m50\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m inference.run(text, T, k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mInference.run\u001b[39m\u001b[34m(self, text, T, k, mode)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mnext\u001b[39m = \u001b[38;5;28mself\u001b[39m.next_token(tokens, T, k,)\n\u001b[32m     31\u001b[39m tokens = torch.cat([tokens, \u001b[38;5;28mnext\u001b[39m.reshape(\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m text = tokenizer.decode(tokens[\u001b[32m0\u001b[39m].tolist())\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.display.update(text)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mnext\u001b[39m[\u001b[32m0\u001b[39m] == tokenizer.token_to_idx[\u001b[33m\"\u001b[39m\u001b[33m</s>\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "text = \"<s><h>A magical horse was spotted in England. It was found in a field. The magical horse is able to breath fire. Scientists don't know where it came from.<b>\"\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "inference.run(text, T, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c0f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
