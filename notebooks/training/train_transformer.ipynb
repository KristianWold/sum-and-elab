{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_kristianwold.transformer import Transformer\n",
    "from transformer_kristianwold.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from transformer_kristianwold.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token id: 24070\n",
      "Vocab size: 24074\n"
     ]
    }
   ],
   "source": [
    "tokenizer = loader(\"../../tokenizers/cnn_tokenizer3.pkl\")\n",
    "\n",
    "start_token_id=tokenizer.token_to_idx[\"<s>\"]\n",
    "vocab_size=tokenizer.vocab_size\n",
    "\n",
    "print(\"Start token id:\", start_token_id)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train1 = loader(\"../../corpus/cnn_dailymail_highlight_first_train.pkl\")\n",
    "corpus_train2 = loader(\"../../corpus/cnn_dailymail_highlight_last_train.pkl\")\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2), dim=0)\n",
    "\n",
    "corpus_test1 = loader(\"../../corpus/cnn_dailymail_highlight_first_test.pkl\")\n",
    "corpus_test2 = loader(\"../../corpus/cnn_dailymail_highlight_last_test.pkl\")\n",
    "corpus_test = torch.cat((corpus_test1, corpus_test2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 315778058\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=start_token_id,\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "filename = \"../../models/checkpoint_transformer_5epoch.pth\"\n",
    "\n",
    "print(\"Number of parameters:\", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "\n",
    "num_epochs      = 5\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ee96f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafe8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint(model, \n",
    "#                 optimizer, \n",
    "#                 scheduler,\n",
    "#                 loss_train_list,\n",
    "#                 loss_test_list, \n",
    "#                 filename=\"models/checkpoint_transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7c59",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[model, \n",
    "# optimizer, \n",
    "# scheduler, \n",
    "# loss_train_list, \n",
    "# loss_test_list] = load_checkpoint(\"../models/checkpoint_transformer_4epoch.pth\", \n",
    "#                                   model, \n",
    "#                                   optimizer, \n",
    "#                                   scheduler, \n",
    "#                                   loss_train_list, \n",
    "#                                   loss_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969cbae5d6644c669b2487fdc9aee851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 3.063, Loss_eval: 3.323, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 3.214, Loss_eval: 3.307, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 3.246, Loss_eval: 3.333, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 3.196, Loss_eval: 3.303, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 3.063, Loss_eval: 3.267, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 3.022, Loss_eval: 3.297, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 2.929, Loss_eval: 3.299, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 3.256, Loss_eval: 3.326, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 3.198, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 3.287, Loss_eval: 3.312, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 3.225, Loss_eval: 3.310, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 3.276, Loss_eval: 3.327, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 3.225, Loss_eval: 3.349, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 3.168, Loss_eval: 3.323, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.085, Loss_eval: 3.272, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 3.484, Loss_eval: 3.323, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 3.187, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 3.220, Loss_eval: 3.324, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 3.241, Loss_eval: 3.291, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 3.222, Loss_eval: 3.283, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 3.242, Loss_eval: 3.310, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 3.225, Loss_eval: 3.282, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.190, Loss_eval: 3.341, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.104, Loss_eval: 3.328, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 3.304, Loss_eval: 3.325, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 3.207, Loss_eval: 3.312, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.131, Loss_eval: 3.252, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 3.020, Loss_eval: 3.313, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 3.502, Loss_eval: 3.326, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 3.465, Loss_eval: 3.284, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 3.265, Loss_eval: 3.344, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.167, Loss_eval: 3.343, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.250, Loss_eval: 3.261, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 2.996, Loss_eval: 3.273, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 3.030, Loss_eval: 3.254, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.196, Loss_eval: 3.317, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 3.163, Loss_eval: 3.314, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 3.291, Loss_eval: 3.341, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 3.460, Loss_eval: 3.276, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 3.303, Loss_eval: 3.285, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 3.003, Loss_eval: 3.268, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 2.949, Loss_eval: 3.328, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 3.183, Loss_eval: 3.339, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.165, Loss_eval: 3.295, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 3.392, Loss_eval: 3.298, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 2.966, Loss_eval: 3.342, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.195, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 3.219, Loss_eval: 3.342, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 3.298, Loss_eval: 3.321, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 3.346, Loss_eval: 3.292, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 3.057, Loss_eval: 3.327, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.189, Loss_eval: 3.322, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 3.308, Loss_eval: 3.256, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 2.983, Loss_eval: 3.275, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 3.396, Loss_eval: 3.283, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.116, Loss_eval: 3.295, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 3.206, Loss_eval: 3.331, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 3.147, Loss_eval: 3.303, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.189, Loss_eval: 3.326, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 3.367, Loss_eval: 3.352, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.079, Loss_eval: 3.360, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 3.418, Loss_eval: 3.306, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.255, Loss_eval: 3.280, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 3.369, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.065, Loss_eval: 3.316, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 2.937, Loss_eval: 3.289, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.287, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.094, Loss_eval: 3.313, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.220, Loss_eval: 3.323, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.266, Loss_eval: 3.298, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 3.370, Loss_eval: 3.262, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 2.924, Loss_eval: 3.234, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.154, Loss_eval: 3.280, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.103, Loss_eval: 3.318, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 3.287, Loss_eval: 3.272, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.079, Loss_eval: 3.266, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 3.598, Loss_eval: 3.315, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.138, Loss_eval: 3.334, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 3.505, Loss_eval: 3.262, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 3.307, Loss_eval: 3.265, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.175, Loss_eval: 3.225, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 3.049, Loss_eval: 3.338, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 3.036, Loss_eval: 3.259, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.382, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.239, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.127, Loss_eval: 3.271, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 3.258, Loss_eval: 3.309, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.076, Loss_eval: 3.268, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.215, Loss_eval: 3.384, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.115, Loss_eval: 3.265, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 3.057, Loss_eval: 3.242, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 3.126, Loss_eval: 3.323, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.289, Loss_eval: 3.316, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 3.328, Loss_eval: 3.297, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 2.872, Loss_eval: 3.264, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.116, Loss_eval: 3.273, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 2.691, Loss_eval: 3.272, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 3.423, Loss_eval: 3.308, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 3.154, Loss_eval: 3.281, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.124, Loss_eval: 3.291, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 3.327, Loss_eval: 3.327, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.146, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 3.258, Loss_eval: 3.305, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.138, Loss_eval: 3.272, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 3.407, Loss_eval: 3.322, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.354, Loss_eval: 3.257, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.196, Loss_eval: 3.277, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 3.037, Loss_eval: 3.254, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 3.309, Loss_eval: 3.307, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 3.370, Loss_eval: 3.311, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.293, Loss_eval: 3.264, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 2.896, Loss_eval: 3.306, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 3.651, Loss_eval: 3.243, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.107, Loss_eval: 3.307, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 3.243, Loss_eval: 3.290, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.126, Loss_eval: 3.300, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 3.177, Loss_eval: 3.258, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 3.297, Loss_eval: 3.270, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 2.996, Loss_eval: 3.364, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 3.130, Loss_eval: 3.239, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 3.132, Loss_eval: 3.312, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 2.852, Loss_eval: 3.241, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 3.008, Loss_eval: 3.319, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.170, Loss_eval: 3.306, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 3.063, Loss_eval: 3.268, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.272, Loss_eval: 3.253, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 3.355, Loss_eval: 3.284, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 2.960, Loss_eval: 3.269, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 3.046, Loss_eval: 3.271, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 2.922, Loss_eval: 3.240, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 2.863, Loss_eval: 3.305, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 3.099, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 2.861, Loss_eval: 3.233, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 3.491, Loss_eval: 3.315, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 2.859, Loss_eval: 3.375, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.169, Loss_eval: 3.286, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 3.067, Loss_eval: 3.324, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.114, Loss_eval: 3.298, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 2.971, Loss_eval: 3.315, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 2.995, Loss_eval: 3.298, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.192, Loss_eval: 3.305, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.238, Loss_eval: 3.259, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 2.866, Loss_eval: 3.273, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 3.065, Loss_eval: 3.379, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.056, Loss_eval: 3.272, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.331, Loss_eval: 3.281, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.103, Loss_eval: 3.236, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.212, Loss_eval: 3.246, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.220, Loss_eval: 3.257, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 2.961, Loss_eval: 3.251, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 2.990, Loss_eval: 3.271, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 2.785, Loss_eval: 3.228, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 2.985, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 3.230, Loss_eval: 3.311, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 2.919, Loss_eval: 3.294, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 3.266, Loss_eval: 3.283, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 3.022, Loss_eval: 3.320, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 2.936, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 3.000, Loss_eval: 3.308, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 3.443, Loss_eval: 3.297, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 3.281, Loss_eval: 3.352, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 2.866, Loss_eval: 3.280, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 3.517, Loss_eval: 3.261, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 3.252, Loss_eval: 3.321, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 2.935, Loss_eval: 3.256, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 3.381, Loss_eval: 3.299, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 3.082, Loss_eval: 3.234, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 2.964, Loss_eval: 3.252, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 2.992, Loss_eval: 3.294, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 3.333, Loss_eval: 3.262, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 3.355, Loss_eval: 3.275, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 3.110, Loss_eval: 3.288, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 3.355, Loss_eval: 3.282, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 3.059, Loss_eval: 3.286, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 2.814, Loss_eval: 3.276, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 2.929, Loss_eval: 3.246, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 3.130, Loss_eval: 3.272, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 3.074, Loss_eval: 3.329, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 3.178, Loss_eval: 3.302, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 3.054, Loss_eval: 3.270, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 3.171, Loss_eval: 3.269, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 3.027, Loss_eval: 3.273, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 3.138, Loss_eval: 3.281, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 3.170, Loss_eval: 3.215, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 3.559, Loss_eval: 3.318, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 3.279, Loss_eval: 3.240, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 3.394, Loss_eval: 3.259, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 3.000, Loss_eval: 3.285, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 3.295, Loss_eval: 3.274, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 3.167, Loss_eval: 3.298, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 3.052, Loss_eval: 3.269, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 2.990, Loss_eval: 3.249, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 3.177, Loss_eval: 3.307, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 3.063, Loss_eval: 3.223, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 3.273, Loss_eval: 3.291, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 2.758, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 3.006, Loss_eval: 3.262, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 2.853, Loss_eval: 3.226, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 3.250, Loss_eval: 3.258, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 2.914, Loss_eval: 3.316, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 3.080, Loss_eval: 3.295, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 3.189, Loss_eval: 3.324, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 2.980, Loss_eval: 3.324, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 3.133, Loss_eval: 3.288, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 3.335, Loss_eval: 3.320, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 3.298, Loss_eval: 3.269, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 3.188, Loss_eval: 3.287, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 3.079, Loss_eval: 3.266, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 3.328, Loss_eval: 3.270, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 2.986, Loss_eval: 3.247, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 2.930, Loss_eval: 3.238, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 3.131, Loss_eval: 3.247, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 2.848, Loss_eval: 3.257, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 2.973, Loss_eval: 3.305, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 3.313, Loss_eval: 3.207, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 2.757, Loss_eval: 3.292, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 2.835, Loss_eval: 3.252, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 2.793, Loss_eval: 3.304, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 3.047, Loss_eval: 3.291, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 3.221, Loss_eval: 3.313, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 3.090, Loss_eval: 3.261, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 3.439, Loss_eval: 3.317, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 2.903, Loss_eval: 3.292, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 3.022, Loss_eval: 3.239, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 3.154, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 3.189, Loss_eval: 3.249, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 3.135, Loss_eval: 3.275, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 3.222, Loss_eval: 3.282, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 3.739, Loss_eval: 3.255, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 3.133, Loss_eval: 3.279, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 3.253, Loss_eval: 3.244, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 3.335, Loss_eval: 3.230, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 3.069, Loss_eval: 3.284, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 3.190, Loss_eval: 3.280, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 3.258, Loss_eval: 3.227, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 3.050, Loss_eval: 3.262, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 3.445, Loss_eval: 3.242, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 3.301, Loss_eval: 3.264, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 3.203, Loss_eval: 3.245, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 2.964, Loss_eval: 3.252, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 3.314, Loss_eval: 3.292, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 3.082, Loss_eval: 3.281, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 3.241, Loss_eval: 3.291, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 3.230, Loss_eval: 3.255, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 3.247, Loss_eval: 3.293, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 2.980, Loss_eval: 3.259, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 3.072, Loss_eval: 3.253, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 3.482, Loss_eval: 3.246, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 2.918, Loss_eval: 3.284, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 3.076, Loss_eval: 3.250, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 2.979, Loss_eval: 3.267, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 3.298, Loss_eval: 3.288, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 2.998, Loss_eval: 3.231, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 2.961, Loss_eval: 3.261, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 2.818, Loss_eval: 3.277, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 2.922, Loss_eval: 3.226, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 3.081, Loss_eval: 3.260, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 3.088, Loss_eval: 3.235, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 3.176, Loss_eval: 3.245, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 3.442, Loss_eval: 3.253, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 2.955, Loss_eval: 3.267, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 3.030, Loss_eval: 3.255, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 3.026, Loss_eval: 3.233, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 3.274, Loss_eval: 3.250, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 3.162, Loss_eval: 3.244, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 2.736, Loss_eval: 3.226, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 3.468, Loss_eval: 3.262, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 3.126, Loss_eval: 3.321, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 3.314, Loss_eval: 3.279, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 3.194, Loss_eval: 3.242, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 3.090, Loss_eval: 3.280, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 3.012, Loss_eval: 3.225, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 3.457, Loss_eval: 3.259, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 3.230, Loss_eval: 3.284, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 3.059, Loss_eval: 3.283, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 2.883, Loss_eval: 3.271, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 3.407, Loss_eval: 3.247, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 3.049, Loss_eval: 3.264, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 3.428, Loss_eval: 3.230, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 3.224, Loss_eval: 3.196, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 3.095, Loss_eval: 3.225, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 3.090, Loss_eval: 3.235, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 3.233, Loss_eval: 3.251, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 3.276, Loss_eval: 3.267, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 3.039, Loss_eval: 3.225, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 2.950, Loss_eval: 3.233, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 3.171, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 3.268, Loss_eval: 3.249, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 2.899, Loss_eval: 3.192, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 3.079, Loss_eval: 3.271, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 3.239, Loss_eval: 3.250, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 3.417, Loss_eval: 3.290, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 3.108, Loss_eval: 3.269, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 2.998, Loss_eval: 3.281, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 3.117, Loss_eval: 3.224, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 3.346, Loss_eval: 3.292, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 3.166, Loss_eval: 3.280, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 3.398, Loss_eval: 3.264, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 2.914, Loss_eval: 3.237, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 3.120, Loss_eval: 3.279, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 2.923, Loss_eval: 3.243, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 3.570, Loss_eval: 3.245, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 3.300, Loss_eval: 3.306, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 3.074, Loss_eval: 3.239, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 3.030, Loss_eval: 3.225, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 3.482, Loss_eval: 3.258, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 3.077, Loss_eval: 3.251, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 3.119, Loss_eval: 3.331, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 3.102, Loss_eval: 3.191, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 3.400, Loss_eval: 3.306, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.082, Loss_eval: 3.304, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 3.326, Loss_eval: 3.296, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.059, Loss_eval: 3.225, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 3.432, Loss_eval: 3.246, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 3.300, Loss_eval: 3.239, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 2.922, Loss_eval: 3.232, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 2.875, Loss_eval: 3.258, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 3.062, Loss_eval: 3.245, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 3.235, Loss_eval: 3.222, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 3.100, Loss_eval: 3.261, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 2.927, Loss_eval: 3.225, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.089, Loss_eval: 3.240, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.077, Loss_eval: 3.309, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 3.130, Loss_eval: 3.274, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 3.242, Loss_eval: 3.288, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 3.115, Loss_eval: 3.239, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 3.177, Loss_eval: 3.207, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 2.973, Loss_eval: 3.243, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 2.997, Loss_eval: 3.244, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 3.082, Loss_eval: 3.220, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 2.902, Loss_eval: 3.252, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 3.021, Loss_eval: 3.229, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 3.030, Loss_eval: 3.283, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 3.019, Loss_eval: 3.282, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 3.429, Loss_eval: 3.233, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 3.261, Loss_eval: 3.280, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 3.225, Loss_eval: 3.258, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 3.131, Loss_eval: 3.253, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 3.248, Loss_eval: 3.289, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 2.920, Loss_eval: 3.281, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 2.896, Loss_eval: 3.255, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 3.323, Loss_eval: 3.318, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 2.873, Loss_eval: 3.243, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 3.052, Loss_eval: 3.264, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 3.401, Loss_eval: 3.228, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.090, Loss_eval: 3.184, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 3.143, Loss_eval: 3.257, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 2.876, Loss_eval: 3.217, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 3.265, Loss_eval: 3.239, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 2.990, Loss_eval: 3.208, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
