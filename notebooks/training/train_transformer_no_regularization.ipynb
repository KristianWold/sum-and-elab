{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_kristianwold.transformer import Transformer\n",
    "from transformer_kristianwold.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from transformer_kristianwold.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token id: 24070\n",
      "Vocab size: 24074\n"
     ]
    }
   ],
   "source": [
    "tokenizer = loader(\"../../tokenizers/cnn_tokenizer3.pkl\")\n",
    "\n",
    "start_token_id=tokenizer.token_to_idx[\"<s>\"]\n",
    "vocab_size=tokenizer.vocab_size\n",
    "\n",
    "print(\"Start token id:\", start_token_id)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train1 = loader(\"../../corpus/cnn_dailymail_highlight_first_train.pkl\")\n",
    "corpus_train2 = loader(\"../../corpus/cnn_dailymail_highlight_last_train.pkl\")\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2), dim=0)\n",
    "\n",
    "corpus_test1 = loader(\"../../corpus/cnn_dailymail_highlight_first_test.pkl\")\n",
    "corpus_test2 = loader(\"../../corpus/cnn_dailymail_highlight_last_test.pkl\")\n",
    "corpus_test = torch.cat((corpus_test1, corpus_test2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 315778058\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.,   # no dropout\n",
    "    start_token_id=start_token_id,\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0., #no weight decay\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "filename = \"../../models/checkpoint_transformer_no_regularization_1epoch.pth\"\n",
    "\n",
    "print(\"Number of parameters:\", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ee96f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cafe8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint(model, \n",
    "#                 optimizer, \n",
    "#                 scheduler,\n",
    "#                 loss_train_list,\n",
    "#                 loss_test_list, \n",
    "#                 filename=\"models/checkpoint_transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7c59",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[model, \n",
    "#optimizer, \n",
    "#scheduler, \n",
    "#loss_train_list, \n",
    "#loss_test_list] = load_checkpoint(\"../../models/checkpoint_transformer_3epoch.pth\", \n",
    "#                                  model, \n",
    "#                                  optimizer, \n",
    "#                                  scheduler, \n",
    "#                                  loss_train_list, \n",
    "#                                  loss_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "419ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4236bf9a98464c9714ed353cb4b809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 10.028, Loss_eval: 10.034, Learning Rate: 6.000000e-07\n",
      "Step 1000, Loss: 9.842, Loss_eval: 9.838, Learning Rate: 1.250000e-06\n",
      "Step 1500, Loss: 9.666, Loss_eval: 9.648, Learning Rate: 1.850000e-06\n",
      "Step 2000, Loss: 9.444, Loss_eval: 9.482, Learning Rate: 2.500000e-06\n",
      "Step 2500, Loss: 9.373, Loss_eval: 9.383, Learning Rate: 3.100000e-06\n",
      "Step 3000, Loss: 9.250, Loss_eval: 9.305, Learning Rate: 3.750000e-06\n",
      "Step 3500, Loss: 9.233, Loss_eval: 9.230, Learning Rate: 4.350000e-06\n",
      "Step 4000, Loss: 9.156, Loss_eval: 9.163, Learning Rate: 5.000000e-06\n",
      "Step 4500, Loss: 9.034, Loss_eval: 9.072, Learning Rate: 5.600000e-06\n",
      "Step 5000, Loss: 8.974, Loss_eval: 8.964, Learning Rate: 6.250000e-06\n",
      "Step 5500, Loss: 8.841, Loss_eval: 8.866, Learning Rate: 6.850000e-06\n",
      "Step 6000, Loss: 8.749, Loss_eval: 8.748, Learning Rate: 7.500000e-06\n",
      "Step 6500, Loss: 8.605, Loss_eval: 8.636, Learning Rate: 8.100000e-06\n",
      "Step 7000, Loss: 8.485, Loss_eval: 8.504, Learning Rate: 8.750000e-06\n",
      "Step 7500, Loss: 8.299, Loss_eval: 8.370, Learning Rate: 9.350000e-06\n",
      "Step 8000, Loss: 8.247, Loss_eval: 8.221, Learning Rate: 1.000000e-05\n",
      "Step 8500, Loss: 8.049, Loss_eval: 8.121, Learning Rate: 1.060000e-05\n",
      "Step 9000, Loss: 7.912, Loss_eval: 7.976, Learning Rate: 1.125000e-05\n",
      "Step 9500, Loss: 7.840, Loss_eval: 7.854, Learning Rate: 1.185000e-05\n",
      "Step 10000, Loss: 7.669, Loss_eval: 7.744, Learning Rate: 1.250000e-05\n",
      "Step 10500, Loss: 7.742, Loss_eval: 7.648, Learning Rate: 1.310000e-05\n",
      "Step 11000, Loss: 7.429, Loss_eval: 7.540, Learning Rate: 1.375000e-05\n",
      "Step 11500, Loss: 7.330, Loss_eval: 7.489, Learning Rate: 1.435000e-05\n",
      "Step 12000, Loss: 7.120, Loss_eval: 7.392, Learning Rate: 1.500000e-05\n",
      "Step 12500, Loss: 7.192, Loss_eval: 7.334, Learning Rate: 1.560000e-05\n",
      "Step 13000, Loss: 7.073, Loss_eval: 7.275, Learning Rate: 1.625000e-05\n",
      "Step 13500, Loss: 6.941, Loss_eval: 7.184, Learning Rate: 1.685000e-05\n",
      "Step 14000, Loss: 7.126, Loss_eval: 7.134, Learning Rate: 1.750000e-05\n",
      "Step 14500, Loss: 7.110, Loss_eval: 7.102, Learning Rate: 1.810000e-05\n",
      "Step 15000, Loss: 6.833, Loss_eval: 7.034, Learning Rate: 1.875000e-05\n",
      "Step 15500, Loss: 6.802, Loss_eval: 6.975, Learning Rate: 1.935000e-05\n",
      "Step 16000, Loss: 6.826, Loss_eval: 6.945, Learning Rate: 2.000000e-05\n",
      "Step 16500, Loss: 6.768, Loss_eval: 6.863, Learning Rate: 2.060000e-05\n",
      "Step 17000, Loss: 6.662, Loss_eval: 6.804, Learning Rate: 2.125000e-05\n",
      "Step 17500, Loss: 6.635, Loss_eval: 6.769, Learning Rate: 2.185000e-05\n",
      "Step 18000, Loss: 6.524, Loss_eval: 6.741, Learning Rate: 2.250000e-05\n",
      "Step 18500, Loss: 6.522, Loss_eval: 6.682, Learning Rate: 2.310000e-05\n",
      "Step 19000, Loss: 6.651, Loss_eval: 6.671, Learning Rate: 2.375000e-05\n",
      "Step 19500, Loss: 6.532, Loss_eval: 6.605, Learning Rate: 2.435000e-05\n",
      "Step 20000, Loss: 6.640, Loss_eval: 6.576, Learning Rate: 2.500000e-05\n",
      "Step 20500, Loss: 6.517, Loss_eval: 6.595, Learning Rate: 2.560000e-05\n",
      "Step 21000, Loss: 6.560, Loss_eval: 6.528, Learning Rate: 2.625000e-05\n",
      "Step 21500, Loss: 6.405, Loss_eval: 6.506, Learning Rate: 2.685000e-05\n",
      "Step 22000, Loss: 6.322, Loss_eval: 6.442, Learning Rate: 2.750000e-05\n",
      "Step 22500, Loss: 6.457, Loss_eval: 6.429, Learning Rate: 2.810000e-05\n",
      "Step 23000, Loss: 6.224, Loss_eval: 6.423, Learning Rate: 2.875000e-05\n",
      "Step 23500, Loss: 6.334, Loss_eval: 6.371, Learning Rate: 2.935000e-05\n",
      "Step 24000, Loss: 6.316, Loss_eval: 6.369, Learning Rate: 3.000000e-05\n",
      "Step 24500, Loss: 6.287, Loss_eval: 6.347, Learning Rate: 3.060000e-05\n",
      "Step 25000, Loss: 6.245, Loss_eval: 6.333, Learning Rate: 3.125000e-05\n",
      "Step 25500, Loss: 6.139, Loss_eval: 6.296, Learning Rate: 3.185000e-05\n",
      "Step 26000, Loss: 6.385, Loss_eval: 6.250, Learning Rate: 3.250000e-05\n",
      "Step 26500, Loss: 6.191, Loss_eval: 6.250, Learning Rate: 3.310000e-05\n",
      "Step 27000, Loss: 6.155, Loss_eval: 6.254, Learning Rate: 3.375000e-05\n",
      "Step 27500, Loss: 6.424, Loss_eval: 6.238, Learning Rate: 3.435000e-05\n",
      "Step 28000, Loss: 6.127, Loss_eval: 6.216, Learning Rate: 3.500000e-05\n",
      "Step 28500, Loss: 6.277, Loss_eval: 6.216, Learning Rate: 3.560000e-05\n",
      "Step 29000, Loss: 5.895, Loss_eval: 6.159, Learning Rate: 3.625000e-05\n",
      "Step 29500, Loss: 6.139, Loss_eval: 6.181, Learning Rate: 3.685000e-05\n",
      "Step 30000, Loss: 5.901, Loss_eval: 6.163, Learning Rate: 3.750000e-05\n",
      "Step 30500, Loss: 5.963, Loss_eval: 6.138, Learning Rate: 3.810000e-05\n",
      "Step 31000, Loss: 6.090, Loss_eval: 6.082, Learning Rate: 3.875000e-05\n",
      "Step 31500, Loss: 6.106, Loss_eval: 6.109, Learning Rate: 3.935000e-05\n",
      "Step 32000, Loss: 6.076, Loss_eval: 6.051, Learning Rate: 4.000000e-05\n",
      "Step 32500, Loss: 5.783, Loss_eval: 6.049, Learning Rate: 4.060000e-05\n",
      "Step 33000, Loss: 5.850, Loss_eval: 6.030, Learning Rate: 4.125000e-05\n",
      "Step 33500, Loss: 5.836, Loss_eval: 6.025, Learning Rate: 4.185000e-05\n",
      "Step 34000, Loss: 5.908, Loss_eval: 6.011, Learning Rate: 4.250000e-05\n",
      "Step 34500, Loss: 6.114, Loss_eval: 6.000, Learning Rate: 4.310000e-05\n",
      "Step 35000, Loss: 5.885, Loss_eval: 5.991, Learning Rate: 4.375000e-05\n",
      "Step 35500, Loss: 6.170, Loss_eval: 5.956, Learning Rate: 4.435000e-05\n",
      "Step 36000, Loss: 5.836, Loss_eval: 5.916, Learning Rate: 4.500000e-05\n",
      "Step 36500, Loss: 5.866, Loss_eval: 5.931, Learning Rate: 4.560000e-05\n",
      "Step 37000, Loss: 5.811, Loss_eval: 5.891, Learning Rate: 4.625000e-05\n",
      "Step 37500, Loss: 5.900, Loss_eval: 5.867, Learning Rate: 4.685000e-05\n",
      "Step 38000, Loss: 5.768, Loss_eval: 5.869, Learning Rate: 4.750000e-05\n",
      "Step 38500, Loss: 5.933, Loss_eval: 5.857, Learning Rate: 4.810000e-05\n",
      "Step 39000, Loss: 5.699, Loss_eval: 5.841, Learning Rate: 4.875000e-05\n",
      "Step 39500, Loss: 5.880, Loss_eval: 5.767, Learning Rate: 4.935000e-05\n",
      "Step 40000, Loss: 6.128, Loss_eval: 5.755, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 5.553, Loss_eval: 5.768, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 5.427, Loss_eval: 5.779, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 5.465, Loss_eval: 5.774, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 5.536, Loss_eval: 5.747, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 5.651, Loss_eval: 5.695, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 5.503, Loss_eval: 5.689, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 5.542, Loss_eval: 5.687, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 5.555, Loss_eval: 5.646, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 5.634, Loss_eval: 5.685, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 5.319, Loss_eval: 5.576, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 5.445, Loss_eval: 5.580, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 5.379, Loss_eval: 5.593, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 5.394, Loss_eval: 5.597, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 5.576, Loss_eval: 5.548, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 5.289, Loss_eval: 5.511, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 5.488, Loss_eval: 5.474, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 5.280, Loss_eval: 5.478, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 5.599, Loss_eval: 5.479, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 5.307, Loss_eval: 5.446, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 5.342, Loss_eval: 5.453, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 5.424, Loss_eval: 5.474, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 5.613, Loss_eval: 5.422, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 5.568, Loss_eval: 5.469, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 5.218, Loss_eval: 5.408, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 5.410, Loss_eval: 5.414, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 5.307, Loss_eval: 5.354, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 5.352, Loss_eval: 5.363, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 5.462, Loss_eval: 5.357, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 5.245, Loss_eval: 5.347, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 5.631, Loss_eval: 5.318, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 5.430, Loss_eval: 5.337, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 4.910, Loss_eval: 5.339, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 5.427, Loss_eval: 5.284, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 4.886, Loss_eval: 5.297, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 5.220, Loss_eval: 5.306, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 5.058, Loss_eval: 5.264, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 5.119, Loss_eval: 5.248, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 5.159, Loss_eval: 5.262, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 5.084, Loss_eval: 5.281, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 5.227, Loss_eval: 5.224, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 5.026, Loss_eval: 5.198, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 5.283, Loss_eval: 5.187, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 5.152, Loss_eval: 5.203, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 5.106, Loss_eval: 5.188, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 5.103, Loss_eval: 5.167, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 5.102, Loss_eval: 5.159, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 5.288, Loss_eval: 5.134, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 5.053, Loss_eval: 5.134, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 4.977, Loss_eval: 5.162, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 4.801, Loss_eval: 5.109, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 5.135, Loss_eval: 5.123, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 5.001, Loss_eval: 5.108, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 4.812, Loss_eval: 5.081, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 5.190, Loss_eval: 5.116, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 4.858, Loss_eval: 5.156, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 4.935, Loss_eval: 5.076, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 4.836, Loss_eval: 5.123, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 4.868, Loss_eval: 5.079, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 4.825, Loss_eval: 5.093, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 4.949, Loss_eval: 5.033, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 4.940, Loss_eval: 5.025, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 5.012, Loss_eval: 4.988, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 4.769, Loss_eval: 5.020, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 4.845, Loss_eval: 5.069, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 4.828, Loss_eval: 4.995, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 4.973, Loss_eval: 5.008, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 4.849, Loss_eval: 4.963, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 5.020, Loss_eval: 4.952, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 4.693, Loss_eval: 4.978, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 4.648, Loss_eval: 4.995, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 4.819, Loss_eval: 4.948, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 4.697, Loss_eval: 4.913, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 4.666, Loss_eval: 4.948, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 4.964, Loss_eval: 5.006, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 4.690, Loss_eval: 4.934, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 5.081, Loss_eval: 4.898, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 4.752, Loss_eval: 4.927, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 4.639, Loss_eval: 4.904, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 4.631, Loss_eval: 4.902, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 5.271, Loss_eval: 4.898, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 5.172, Loss_eval: 4.915, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 4.562, Loss_eval: 4.872, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 4.943, Loss_eval: 4.840, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 4.901, Loss_eval: 4.887, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 4.750, Loss_eval: 4.826, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 4.871, Loss_eval: 4.876, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 4.656, Loss_eval: 4.796, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 4.672, Loss_eval: 4.832, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 4.692, Loss_eval: 4.824, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 5.044, Loss_eval: 4.811, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 4.864, Loss_eval: 4.818, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 4.458, Loss_eval: 4.778, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 4.883, Loss_eval: 4.793, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 4.776, Loss_eval: 4.758, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 4.273, Loss_eval: 4.771, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 4.461, Loss_eval: 4.742, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 4.664, Loss_eval: 4.803, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 4.627, Loss_eval: 4.765, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 4.817, Loss_eval: 4.755, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 4.476, Loss_eval: 4.714, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 4.725, Loss_eval: 4.707, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 4.761, Loss_eval: 4.700, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 4.706, Loss_eval: 4.706, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 4.495, Loss_eval: 4.670, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 5.110, Loss_eval: 4.729, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 4.577, Loss_eval: 4.663, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 4.806, Loss_eval: 4.713, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 4.446, Loss_eval: 4.691, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 4.740, Loss_eval: 4.649, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 4.656, Loss_eval: 4.688, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 4.427, Loss_eval: 4.661, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 4.471, Loss_eval: 4.620, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 4.628, Loss_eval: 4.670, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 4.605, Loss_eval: 4.600, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 4.617, Loss_eval: 4.663, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 4.079, Loss_eval: 4.604, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 4.611, Loss_eval: 4.576, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 4.159, Loss_eval: 4.517, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 4.709, Loss_eval: 4.556, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 4.198, Loss_eval: 4.592, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 4.434, Loss_eval: 4.580, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 4.539, Loss_eval: 4.598, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 4.398, Loss_eval: 4.566, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 4.457, Loss_eval: 4.527, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 4.634, Loss_eval: 4.579, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 4.525, Loss_eval: 4.518, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 4.541, Loss_eval: 4.514, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 4.308, Loss_eval: 4.495, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 4.677, Loss_eval: 4.502, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 4.507, Loss_eval: 4.474, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 4.329, Loss_eval: 4.462, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 4.492, Loss_eval: 4.463, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 4.174, Loss_eval: 4.459, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 4.247, Loss_eval: 4.493, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 4.594, Loss_eval: 4.399, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 3.957, Loss_eval: 4.460, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 3.962, Loss_eval: 4.431, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 3.983, Loss_eval: 4.474, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 4.218, Loss_eval: 4.455, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 4.453, Loss_eval: 4.465, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 4.305, Loss_eval: 4.420, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 4.666, Loss_eval: 4.456, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 4.219, Loss_eval: 4.412, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 4.284, Loss_eval: 4.390, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 4.430, Loss_eval: 4.438, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 4.338, Loss_eval: 4.389, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 4.328, Loss_eval: 4.390, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 4.496, Loss_eval: 4.375, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 4.943, Loss_eval: 4.380, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 4.228, Loss_eval: 4.385, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 4.607, Loss_eval: 4.354, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 4.483, Loss_eval: 4.314, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 4.208, Loss_eval: 4.376, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 4.391, Loss_eval: 4.377, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 4.464, Loss_eval: 4.314, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 4.212, Loss_eval: 4.334, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 4.613, Loss_eval: 4.333, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 4.388, Loss_eval: 4.322, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 4.390, Loss_eval: 4.318, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 4.108, Loss_eval: 4.312, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 4.434, Loss_eval: 4.363, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 4.157, Loss_eval: 4.333, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 4.519, Loss_eval: 4.310, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 4.304, Loss_eval: 4.305, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 4.347, Loss_eval: 4.317, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 4.097, Loss_eval: 4.274, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 4.174, Loss_eval: 4.272, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 4.640, Loss_eval: 4.257, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 4.103, Loss_eval: 4.289, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 4.143, Loss_eval: 4.257, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 4.156, Loss_eval: 4.270, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 4.380, Loss_eval: 4.306, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 4.119, Loss_eval: 4.244, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 4.109, Loss_eval: 4.272, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 4.007, Loss_eval: 4.255, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 4.008, Loss_eval: 4.225, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 3.984, Loss_eval: 4.251, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 4.135, Loss_eval: 4.235, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 4.165, Loss_eval: 4.231, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 4.385, Loss_eval: 4.222, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 4.132, Loss_eval: 4.233, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 4.061, Loss_eval: 4.219, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 4.140, Loss_eval: 4.203, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 4.205, Loss_eval: 4.216, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 4.230, Loss_eval: 4.207, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 3.652, Loss_eval: 4.174, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 4.450, Loss_eval: 4.213, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 4.199, Loss_eval: 4.250, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 4.433, Loss_eval: 4.214, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 4.171, Loss_eval: 4.182, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 4.096, Loss_eval: 4.203, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 3.977, Loss_eval: 4.158, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 4.551, Loss_eval: 4.194, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 4.259, Loss_eval: 4.205, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 4.084, Loss_eval: 4.184, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 3.871, Loss_eval: 4.202, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 4.300, Loss_eval: 4.173, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 4.063, Loss_eval: 4.175, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 4.400, Loss_eval: 4.147, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 4.257, Loss_eval: 4.096, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 4.084, Loss_eval: 4.153, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 4.100, Loss_eval: 4.126, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 4.216, Loss_eval: 4.148, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 4.181, Loss_eval: 4.159, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 4.092, Loss_eval: 4.117, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 3.957, Loss_eval: 4.121, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 4.098, Loss_eval: 4.180, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 4.225, Loss_eval: 4.131, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 3.934, Loss_eval: 4.071, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 4.094, Loss_eval: 4.153, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 4.181, Loss_eval: 4.118, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 4.380, Loss_eval: 4.146, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 4.146, Loss_eval: 4.135, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 3.911, Loss_eval: 4.121, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 4.055, Loss_eval: 4.093, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 4.230, Loss_eval: 4.148, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 4.077, Loss_eval: 4.126, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 4.500, Loss_eval: 4.126, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 3.790, Loss_eval: 4.077, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 4.036, Loss_eval: 4.113, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 3.874, Loss_eval: 4.089, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 4.461, Loss_eval: 4.093, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 4.275, Loss_eval: 4.133, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 4.042, Loss_eval: 4.081, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 3.995, Loss_eval: 4.056, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 4.394, Loss_eval: 4.093, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 3.913, Loss_eval: 4.082, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 4.037, Loss_eval: 4.131, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 4.038, Loss_eval: 3.998, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 4.326, Loss_eval: 4.118, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.939, Loss_eval: 4.110, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 4.199, Loss_eval: 4.112, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.955, Loss_eval: 4.042, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 4.287, Loss_eval: 4.071, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 4.214, Loss_eval: 4.059, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 3.842, Loss_eval: 4.031, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 3.798, Loss_eval: 4.054, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 4.033, Loss_eval: 4.048, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 4.099, Loss_eval: 4.021, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 3.926, Loss_eval: 4.052, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 3.854, Loss_eval: 4.012, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.963, Loss_eval: 4.017, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.944, Loss_eval: 4.081, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 4.049, Loss_eval: 4.067, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 4.098, Loss_eval: 4.070, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 4.010, Loss_eval: 3.995, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 4.051, Loss_eval: 3.970, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 3.834, Loss_eval: 4.026, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 3.996, Loss_eval: 4.009, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 3.924, Loss_eval: 4.000, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 3.766, Loss_eval: 4.020, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 3.864, Loss_eval: 3.994, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 3.955, Loss_eval: 4.046, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 3.841, Loss_eval: 4.044, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 4.313, Loss_eval: 3.991, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 4.089, Loss_eval: 4.020, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 4.121, Loss_eval: 4.017, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 3.870, Loss_eval: 4.005, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 4.190, Loss_eval: 4.042, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 3.769, Loss_eval: 4.036, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 3.760, Loss_eval: 4.002, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 4.182, Loss_eval: 4.069, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 3.748, Loss_eval: 3.982, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 3.840, Loss_eval: 4.011, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 4.203, Loss_eval: 3.960, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.882, Loss_eval: 3.912, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 4.030, Loss_eval: 3.992, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 3.670, Loss_eval: 3.934, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 4.002, Loss_eval: 3.972, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 3.810, Loss_eval: 3.918, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e12e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0b209fb9544f478b03789e25bd1d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 4.177, Loss_eval: 3.956, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 3.959, Loss_eval: 3.962, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 3.600, Loss_eval: 3.953, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 3.891, Loss_eval: 3.959, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 4.048, Loss_eval: 3.919, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 4.040, Loss_eval: 3.975, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 3.719, Loss_eval: 3.919, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 3.706, Loss_eval: 3.912, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 4.089, Loss_eval: 3.885, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 3.784, Loss_eval: 3.937, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 3.968, Loss_eval: 3.974, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 4.275, Loss_eval: 4.011, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 3.970, Loss_eval: 3.945, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 4.137, Loss_eval: 3.901, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.783, Loss_eval: 3.878, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 4.082, Loss_eval: 3.899, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 3.983, Loss_eval: 4.019, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 3.972, Loss_eval: 3.940, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 3.994, Loss_eval: 3.903, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 3.976, Loss_eval: 3.949, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 3.918, Loss_eval: 3.932, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 3.998, Loss_eval: 3.865, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.476, Loss_eval: 3.918, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.878, Loss_eval: 3.953, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 3.776, Loss_eval: 3.916, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 3.922, Loss_eval: 3.944, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.470, Loss_eval: 3.935, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 3.927, Loss_eval: 3.910, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 3.517, Loss_eval: 3.936, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 3.672, Loss_eval: 3.857, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 3.589, Loss_eval: 3.947, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.776, Loss_eval: 3.911, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.841, Loss_eval: 3.857, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 4.140, Loss_eval: 3.889, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 3.958, Loss_eval: 3.881, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.820, Loss_eval: 3.913, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 3.729, Loss_eval: 3.876, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 3.864, Loss_eval: 3.964, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 3.903, Loss_eval: 3.924, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 3.849, Loss_eval: 3.843, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 3.811, Loss_eval: 3.867, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 3.809, Loss_eval: 3.889, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 3.735, Loss_eval: 3.868, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.607, Loss_eval: 3.866, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 3.705, Loss_eval: 3.892, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 4.019, Loss_eval: 3.845, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.810, Loss_eval: 3.889, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 4.153, Loss_eval: 3.902, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 3.808, Loss_eval: 3.881, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 3.694, Loss_eval: 3.884, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 3.919, Loss_eval: 3.895, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.619, Loss_eval: 3.855, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 3.688, Loss_eval: 3.887, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 3.820, Loss_eval: 3.845, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 3.801, Loss_eval: 3.828, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.725, Loss_eval: 3.852, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 3.807, Loss_eval: 3.855, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 4.121, Loss_eval: 3.879, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.552, Loss_eval: 3.874, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 4.041, Loss_eval: 3.823, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.588, Loss_eval: 3.837, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 3.947, Loss_eval: 3.827, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.796, Loss_eval: 3.843, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 3.764, Loss_eval: 3.887, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.996, Loss_eval: 3.874, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 4.049, Loss_eval: 3.746, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.577, Loss_eval: 3.860, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.819, Loss_eval: 3.804, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.856, Loss_eval: 3.876, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.733, Loss_eval: 3.806, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 3.927, Loss_eval: 3.859, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 3.823, Loss_eval: 3.809, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.755, Loss_eval: 3.830, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.744, Loss_eval: 3.796, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 3.873, Loss_eval: 3.829, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.795, Loss_eval: 3.900, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 3.508, Loss_eval: 3.824, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.541, Loss_eval: 3.836, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 3.675, Loss_eval: 3.827, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 3.784, Loss_eval: 3.787, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.900, Loss_eval: 3.788, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 4.088, Loss_eval: 3.813, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 3.768, Loss_eval: 3.842, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.821, Loss_eval: 3.834, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.688, Loss_eval: 3.809, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.780, Loss_eval: 3.757, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 3.710, Loss_eval: 3.790, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.656, Loss_eval: 3.823, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.868, Loss_eval: 3.768, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.381, Loss_eval: 3.762, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 3.669, Loss_eval: 3.805, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 3.838, Loss_eval: 3.822, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.762, Loss_eval: 3.810, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 3.772, Loss_eval: 3.807, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 3.787, Loss_eval: 3.791, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.740, Loss_eval: 3.777, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 3.556, Loss_eval: 3.808, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 4.101, Loss_eval: 3.843, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 3.650, Loss_eval: 3.733, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.723, Loss_eval: 3.783, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 4.123, Loss_eval: 3.797, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.937, Loss_eval: 3.751, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 3.694, Loss_eval: 3.821, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.722, Loss_eval: 3.797, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 3.810, Loss_eval: 3.772, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.409, Loss_eval: 3.739, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.669, Loss_eval: 3.759, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 3.321, Loss_eval: 3.797, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 3.924, Loss_eval: 3.791, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 3.449, Loss_eval: 3.777, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.528, Loss_eval: 3.783, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 3.483, Loss_eval: 3.781, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 3.434, Loss_eval: 3.817, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.882, Loss_eval: 3.783, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 3.748, Loss_eval: 3.744, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.674, Loss_eval: 3.802, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 3.685, Loss_eval: 3.741, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 3.935, Loss_eval: 3.753, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 3.621, Loss_eval: 3.710, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 3.621, Loss_eval: 3.755, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 4.094, Loss_eval: 3.739, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 3.493, Loss_eval: 3.771, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 3.811, Loss_eval: 3.741, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.719, Loss_eval: 3.723, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 3.956, Loss_eval: 3.748, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.821, Loss_eval: 3.758, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 3.694, Loss_eval: 3.722, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 4.189, Loss_eval: 3.755, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 3.730, Loss_eval: 3.740, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 3.934, Loss_eval: 3.740, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 3.909, Loss_eval: 3.745, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 4.014, Loss_eval: 3.698, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 3.397, Loss_eval: 3.719, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 3.497, Loss_eval: 3.716, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 3.848, Loss_eval: 3.753, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.534, Loss_eval: 3.807, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 3.677, Loss_eval: 3.712, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.621, Loss_eval: 3.716, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 3.539, Loss_eval: 3.696, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 3.569, Loss_eval: 3.721, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.655, Loss_eval: 3.745, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.633, Loss_eval: 3.756, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 3.697, Loss_eval: 3.717, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 3.596, Loss_eval: 3.656, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.917, Loss_eval: 3.734, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.638, Loss_eval: 3.770, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.638, Loss_eval: 3.715, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.812, Loss_eval: 3.728, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.593, Loss_eval: 3.729, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 3.472, Loss_eval: 3.728, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 3.618, Loss_eval: 3.707, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 3.813, Loss_eval: 3.699, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 3.822, Loss_eval: 3.771, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 3.512, Loss_eval: 3.732, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 3.518, Loss_eval: 3.760, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 3.384, Loss_eval: 3.696, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 3.707, Loss_eval: 3.723, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 3.931, Loss_eval: 3.695, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 3.842, Loss_eval: 3.698, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 3.670, Loss_eval: 3.712, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 3.862, Loss_eval: 3.690, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 3.631, Loss_eval: 3.749, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 3.610, Loss_eval: 3.696, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 3.709, Loss_eval: 3.703, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 3.511, Loss_eval: 3.656, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 3.819, Loss_eval: 3.730, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 3.650, Loss_eval: 3.702, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 3.656, Loss_eval: 3.638, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 3.635, Loss_eval: 3.653, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 3.954, Loss_eval: 3.622, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 3.541, Loss_eval: 3.715, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 3.737, Loss_eval: 3.643, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 3.585, Loss_eval: 3.646, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 3.840, Loss_eval: 3.658, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 3.931, Loss_eval: 3.740, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 3.601, Loss_eval: 3.724, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 3.782, Loss_eval: 3.686, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 3.631, Loss_eval: 3.686, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 3.538, Loss_eval: 3.685, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 3.837, Loss_eval: 3.652, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 3.467, Loss_eval: 3.693, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 3.845, Loss_eval: 3.671, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 3.713, Loss_eval: 3.695, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 3.618, Loss_eval: 3.705, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 3.848, Loss_eval: 3.719, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 3.570, Loss_eval: 3.670, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 3.474, Loss_eval: 3.690, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 3.597, Loss_eval: 3.685, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 3.558, Loss_eval: 3.621, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 3.726, Loss_eval: 3.613, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 3.691, Loss_eval: 3.677, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 3.467, Loss_eval: 3.654, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 3.429, Loss_eval: 3.667, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 3.634, Loss_eval: 3.663, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 3.364, Loss_eval: 3.670, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 3.387, Loss_eval: 3.697, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 3.590, Loss_eval: 3.628, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 3.580, Loss_eval: 3.660, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 3.365, Loss_eval: 3.680, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 3.211, Loss_eval: 3.702, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 3.607, Loss_eval: 3.657, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 3.687, Loss_eval: 3.677, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 3.739, Loss_eval: 3.680, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 3.551, Loss_eval: 3.657, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 3.524, Loss_eval: 3.618, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 3.520, Loss_eval: 3.661, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 4.042, Loss_eval: 3.627, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 3.726, Loss_eval: 3.641, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 3.585, Loss_eval: 3.662, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 3.673, Loss_eval: 3.690, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 3.382, Loss_eval: 3.664, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 3.516, Loss_eval: 3.599, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 3.314, Loss_eval: 3.637, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 3.887, Loss_eval: 3.670, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 3.680, Loss_eval: 3.653, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 3.731, Loss_eval: 3.605, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 3.352, Loss_eval: 3.578, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 3.418, Loss_eval: 3.661, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 3.259, Loss_eval: 3.639, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 3.758, Loss_eval: 3.657, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 3.525, Loss_eval: 3.699, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 3.296, Loss_eval: 3.667, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 3.433, Loss_eval: 3.623, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 3.529, Loss_eval: 3.681, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 3.447, Loss_eval: 3.630, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 3.604, Loss_eval: 3.613, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 3.488, Loss_eval: 3.611, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 3.603, Loss_eval: 3.650, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 3.663, Loss_eval: 3.629, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 3.646, Loss_eval: 3.574, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 4.132, Loss_eval: 3.679, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 3.798, Loss_eval: 3.652, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 3.614, Loss_eval: 3.659, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 3.380, Loss_eval: 3.626, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 3.405, Loss_eval: 3.620, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 3.685, Loss_eval: 3.622, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 3.263, Loss_eval: 3.583, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 3.728, Loss_eval: 3.605, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 3.660, Loss_eval: 3.622, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 4.046, Loss_eval: 3.584, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 3.558, Loss_eval: 3.588, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 3.887, Loss_eval: 3.628, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 3.617, Loss_eval: 3.615, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 3.592, Loss_eval: 3.669, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 3.454, Loss_eval: 3.628, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 3.450, Loss_eval: 3.588, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 3.757, Loss_eval: 3.589, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 3.502, Loss_eval: 3.597, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 3.573, Loss_eval: 3.594, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 3.415, Loss_eval: 3.637, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 3.757, Loss_eval: 3.618, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 3.261, Loss_eval: 3.634, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 3.958, Loss_eval: 3.635, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 3.312, Loss_eval: 3.620, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 3.545, Loss_eval: 3.548, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 3.445, Loss_eval: 3.621, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 3.519, Loss_eval: 3.607, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 3.446, Loss_eval: 3.584, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 3.674, Loss_eval: 3.569, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 3.499, Loss_eval: 3.620, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 3.384, Loss_eval: 3.562, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 3.625, Loss_eval: 3.567, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 3.370, Loss_eval: 3.602, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 3.423, Loss_eval: 3.619, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 3.394, Loss_eval: 3.606, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 3.576, Loss_eval: 3.569, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 3.484, Loss_eval: 3.614, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 3.295, Loss_eval: 3.581, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 3.613, Loss_eval: 3.594, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 3.354, Loss_eval: 3.628, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 3.718, Loss_eval: 3.558, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 3.484, Loss_eval: 3.604, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 3.478, Loss_eval: 3.656, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 3.807, Loss_eval: 3.554, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 3.367, Loss_eval: 3.581, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 3.615, Loss_eval: 3.602, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 3.985, Loss_eval: 3.611, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 3.539, Loss_eval: 3.615, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 3.193, Loss_eval: 3.572, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 3.679, Loss_eval: 3.539, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 3.683, Loss_eval: 3.546, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 3.263, Loss_eval: 3.523, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 3.850, Loss_eval: 3.589, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 3.534, Loss_eval: 3.536, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 3.584, Loss_eval: 3.609, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 3.490, Loss_eval: 3.550, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 3.275, Loss_eval: 3.608, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 3.249, Loss_eval: 3.656, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 3.526, Loss_eval: 3.602, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 3.592, Loss_eval: 3.513, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 3.757, Loss_eval: 3.595, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 3.425, Loss_eval: 3.565, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 3.692, Loss_eval: 3.610, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 3.416, Loss_eval: 3.554, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 3.387, Loss_eval: 3.582, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 3.653, Loss_eval: 3.565, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 3.799, Loss_eval: 3.510, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 3.486, Loss_eval: 3.543, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 3.551, Loss_eval: 3.565, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 3.492, Loss_eval: 3.566, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 3.401, Loss_eval: 3.570, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 3.787, Loss_eval: 3.561, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 3.364, Loss_eval: 3.596, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 3.661, Loss_eval: 3.477, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 3.419, Loss_eval: 3.544, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 3.690, Loss_eval: 3.520, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 3.238, Loss_eval: 3.515, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 3.430, Loss_eval: 3.556, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 3.671, Loss_eval: 3.555, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 3.717, Loss_eval: 3.472, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.580, Loss_eval: 3.586, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 3.400, Loss_eval: 3.497, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.332, Loss_eval: 3.577, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 3.176, Loss_eval: 3.545, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 3.624, Loss_eval: 3.568, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 3.448, Loss_eval: 3.551, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 3.965, Loss_eval: 3.556, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 3.470, Loss_eval: 3.528, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 3.613, Loss_eval: 3.593, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 3.168, Loss_eval: 3.590, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 3.660, Loss_eval: 3.477, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.246, Loss_eval: 3.566, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.815, Loss_eval: 3.530, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 3.444, Loss_eval: 3.514, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 3.466, Loss_eval: 3.622, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 3.489, Loss_eval: 3.532, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 3.568, Loss_eval: 3.549, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 3.278, Loss_eval: 3.534, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 3.313, Loss_eval: 3.588, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 3.262, Loss_eval: 3.555, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 3.344, Loss_eval: 3.542, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 3.599, Loss_eval: 3.534, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 3.407, Loss_eval: 3.533, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 3.371, Loss_eval: 3.548, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 3.222, Loss_eval: 3.566, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 3.366, Loss_eval: 3.491, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 3.610, Loss_eval: 3.531, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 3.567, Loss_eval: 3.498, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 3.389, Loss_eval: 3.502, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 3.737, Loss_eval: 3.599, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 3.045, Loss_eval: 3.524, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 3.161, Loss_eval: 3.527, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 3.544, Loss_eval: 3.525, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 3.184, Loss_eval: 3.509, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 3.571, Loss_eval: 3.523, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.749, Loss_eval: 3.485, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 3.645, Loss_eval: 3.516, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 3.466, Loss_eval: 3.544, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 3.441, Loss_eval: 3.470, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 3.346, Loss_eval: 3.501, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "filename = \"../../models/checkpoint_transformer_no_regularization_2epoch.pth\"\n",
    "\n",
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea79537163a24dbabd118c43e6301ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 3.303, Loss_eval: 3.555, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 3.390, Loss_eval: 3.495, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 3.425, Loss_eval: 3.539, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 3.247, Loss_eval: 3.494, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 3.468, Loss_eval: 3.560, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 3.184, Loss_eval: 3.509, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 3.779, Loss_eval: 3.531, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 3.466, Loss_eval: 3.503, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 3.421, Loss_eval: 3.492, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 3.500, Loss_eval: 3.517, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 3.399, Loss_eval: 3.472, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 3.729, Loss_eval: 3.494, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 3.541, Loss_eval: 3.496, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 3.388, Loss_eval: 3.539, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.485, Loss_eval: 3.484, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 3.510, Loss_eval: 3.518, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 3.401, Loss_eval: 3.503, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 3.349, Loss_eval: 3.525, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 3.272, Loss_eval: 3.528, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 3.736, Loss_eval: 3.507, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 3.327, Loss_eval: 3.455, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 3.367, Loss_eval: 3.552, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.247, Loss_eval: 3.537, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.368, Loss_eval: 3.501, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 3.212, Loss_eval: 3.519, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 3.113, Loss_eval: 3.494, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.319, Loss_eval: 3.502, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 3.532, Loss_eval: 3.486, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 3.614, Loss_eval: 3.473, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 3.041, Loss_eval: 3.512, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 3.444, Loss_eval: 3.482, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.538, Loss_eval: 3.484, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.385, Loss_eval: 3.495, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 3.343, Loss_eval: 3.491, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 3.116, Loss_eval: 3.498, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.213, Loss_eval: 3.523, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 3.458, Loss_eval: 3.557, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 3.597, Loss_eval: 3.480, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 3.322, Loss_eval: 3.425, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 3.731, Loss_eval: 3.476, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 3.548, Loss_eval: 3.476, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 3.398, Loss_eval: 3.498, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 3.771, Loss_eval: 3.496, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.570, Loss_eval: 3.474, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 3.897, Loss_eval: 3.565, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 3.850, Loss_eval: 3.536, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.263, Loss_eval: 3.508, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 3.305, Loss_eval: 3.493, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 3.218, Loss_eval: 3.510, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 3.419, Loss_eval: 3.468, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 3.226, Loss_eval: 3.470, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.133, Loss_eval: 3.505, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 3.144, Loss_eval: 3.483, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 3.268, Loss_eval: 3.447, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 3.460, Loss_eval: 3.518, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.494, Loss_eval: 3.506, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 3.548, Loss_eval: 3.509, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 3.331, Loss_eval: 3.463, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.413, Loss_eval: 3.482, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 3.192, Loss_eval: 3.461, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.336, Loss_eval: 3.510, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 3.305, Loss_eval: 3.509, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.615, Loss_eval: 3.467, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 3.430, Loss_eval: 3.482, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.466, Loss_eval: 3.471, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 3.673, Loss_eval: 3.501, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.449, Loss_eval: 3.482, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.602, Loss_eval: 3.486, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.481, Loss_eval: 3.517, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.203, Loss_eval: 3.465, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 3.357, Loss_eval: 3.494, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 3.282, Loss_eval: 3.482, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.685, Loss_eval: 3.428, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.256, Loss_eval: 3.450, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 3.383, Loss_eval: 3.445, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.254, Loss_eval: 3.514, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 3.190, Loss_eval: 3.504, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.310, Loss_eval: 3.448, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 3.565, Loss_eval: 3.522, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 3.309, Loss_eval: 3.469, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.190, Loss_eval: 3.494, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 3.376, Loss_eval: 3.474, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 3.379, Loss_eval: 3.478, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.678, Loss_eval: 3.482, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.325, Loss_eval: 3.410, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.491, Loss_eval: 3.520, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 3.192, Loss_eval: 3.499, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.221, Loss_eval: 3.474, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.610, Loss_eval: 3.433, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.173, Loss_eval: 3.534, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 3.474, Loss_eval: 3.449, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 3.632, Loss_eval: 3.473, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.207, Loss_eval: 3.459, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 3.297, Loss_eval: 3.467, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 3.273, Loss_eval: 3.442, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.433, Loss_eval: 3.447, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 2.819, Loss_eval: 3.454, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 3.514, Loss_eval: 3.449, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 2.951, Loss_eval: 3.470, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.309, Loss_eval: 3.474, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 3.297, Loss_eval: 3.445, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.462, Loss_eval: 3.542, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 3.563, Loss_eval: 3.448, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.348, Loss_eval: 3.499, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 3.384, Loss_eval: 3.458, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.308, Loss_eval: 3.462, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.326, Loss_eval: 3.476, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 3.155, Loss_eval: 3.437, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 3.237, Loss_eval: 3.435, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 3.157, Loss_eval: 3.513, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.319, Loss_eval: 3.449, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 3.281, Loss_eval: 3.506, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 3.372, Loss_eval: 3.463, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.397, Loss_eval: 3.483, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 3.194, Loss_eval: 3.477, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.166, Loss_eval: 3.470, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 3.457, Loss_eval: 3.490, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 2.959, Loss_eval: 3.471, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 3.371, Loss_eval: 3.449, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 3.350, Loss_eval: 3.465, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 3.142, Loss_eval: 3.493, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 3.225, Loss_eval: 3.446, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 2.906, Loss_eval: 3.442, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.358, Loss_eval: 3.471, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 3.266, Loss_eval: 3.476, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.110, Loss_eval: 3.451, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 3.341, Loss_eval: 3.463, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 3.367, Loss_eval: 3.448, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 3.400, Loss_eval: 3.438, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 3.229, Loss_eval: 3.464, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 3.460, Loss_eval: 3.485, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 3.479, Loss_eval: 3.402, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 3.556, Loss_eval: 3.432, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 3.420, Loss_eval: 3.404, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 3.370, Loss_eval: 3.456, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.385, Loss_eval: 3.424, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 3.496, Loss_eval: 3.411, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.218, Loss_eval: 3.375, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 3.732, Loss_eval: 3.407, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 3.398, Loss_eval: 3.414, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.698, Loss_eval: 3.426, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.347, Loss_eval: 3.457, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 3.029, Loss_eval: 3.458, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 3.521, Loss_eval: 3.453, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.543, Loss_eval: 3.433, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.514, Loss_eval: 3.419, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.280, Loss_eval: 3.418, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.308, Loss_eval: 3.435, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.444, Loss_eval: 3.423, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 3.634, Loss_eval: 3.421, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 3.304, Loss_eval: 3.402, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 3.224, Loss_eval: 3.409, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 3.293, Loss_eval: 3.432, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 3.539, Loss_eval: 3.433, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 3.489, Loss_eval: 3.457, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 3.245, Loss_eval: 3.474, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 3.493, Loss_eval: 3.405, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 3.473, Loss_eval: 3.471, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 3.361, Loss_eval: 3.482, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 3.439, Loss_eval: 3.446, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 3.712, Loss_eval: 3.434, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 3.361, Loss_eval: 3.379, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 3.488, Loss_eval: 3.438, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 3.247, Loss_eval: 3.429, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 3.062, Loss_eval: 3.403, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 3.296, Loss_eval: 3.418, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 3.515, Loss_eval: 3.479, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 3.229, Loss_eval: 3.334, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 3.674, Loss_eval: 3.418, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 3.412, Loss_eval: 3.442, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 3.300, Loss_eval: 3.438, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 3.192, Loss_eval: 3.446, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 3.456, Loss_eval: 3.408, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 3.829, Loss_eval: 3.407, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 3.672, Loss_eval: 3.408, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 3.614, Loss_eval: 3.412, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 3.334, Loss_eval: 3.436, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 3.585, Loss_eval: 3.389, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 3.045, Loss_eval: 3.420, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 3.222, Loss_eval: 3.463, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 3.469, Loss_eval: 3.433, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 3.551, Loss_eval: 3.431, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 3.472, Loss_eval: 3.418, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 3.658, Loss_eval: 3.415, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 3.278, Loss_eval: 3.401, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 3.852, Loss_eval: 3.407, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 3.218, Loss_eval: 3.378, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 3.101, Loss_eval: 3.399, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 3.270, Loss_eval: 3.396, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 3.220, Loss_eval: 3.401, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 3.273, Loss_eval: 3.423, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 3.351, Loss_eval: 3.422, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 3.473, Loss_eval: 3.421, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 3.364, Loss_eval: 3.380, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 3.357, Loss_eval: 3.364, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 3.542, Loss_eval: 3.458, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 3.046, Loss_eval: 3.402, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 3.519, Loss_eval: 3.337, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 3.513, Loss_eval: 3.388, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 3.348, Loss_eval: 3.415, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 3.640, Loss_eval: 3.410, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 3.227, Loss_eval: 3.422, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 3.276, Loss_eval: 3.408, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 2.997, Loss_eval: 3.446, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 3.304, Loss_eval: 3.428, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 3.107, Loss_eval: 3.388, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 3.355, Loss_eval: 3.417, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 3.240, Loss_eval: 3.383, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 3.116, Loss_eval: 3.387, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 2.800, Loss_eval: 3.410, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 3.577, Loss_eval: 3.402, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 3.174, Loss_eval: 3.422, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 3.532, Loss_eval: 3.472, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 3.102, Loss_eval: 3.334, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 3.396, Loss_eval: 3.420, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 3.263, Loss_eval: 3.411, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 3.189, Loss_eval: 3.428, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 3.685, Loss_eval: 3.392, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 3.171, Loss_eval: 3.355, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 3.096, Loss_eval: 3.469, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 3.136, Loss_eval: 3.345, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 3.254, Loss_eval: 3.389, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 3.295, Loss_eval: 3.399, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 3.326, Loss_eval: 3.432, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 3.405, Loss_eval: 3.368, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 3.289, Loss_eval: 3.346, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 3.520, Loss_eval: 3.345, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 3.235, Loss_eval: 3.432, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 3.533, Loss_eval: 3.443, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 3.213, Loss_eval: 3.421, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 3.690, Loss_eval: 3.377, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 3.797, Loss_eval: 3.378, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 3.339, Loss_eval: 3.444, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 3.169, Loss_eval: 3.352, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 3.509, Loss_eval: 3.385, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 3.405, Loss_eval: 3.444, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 3.110, Loss_eval: 3.417, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 3.062, Loss_eval: 3.360, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 3.152, Loss_eval: 3.391, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 3.042, Loss_eval: 3.415, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 3.429, Loss_eval: 3.412, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 3.191, Loss_eval: 3.415, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 3.398, Loss_eval: 3.374, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 3.294, Loss_eval: 3.382, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 3.646, Loss_eval: 3.385, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 3.241, Loss_eval: 3.419, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 3.493, Loss_eval: 3.380, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 3.166, Loss_eval: 3.449, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 3.032, Loss_eval: 3.354, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 3.371, Loss_eval: 3.405, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 3.293, Loss_eval: 3.383, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 3.433, Loss_eval: 3.343, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 3.493, Loss_eval: 3.421, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 3.373, Loss_eval: 3.333, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 3.169, Loss_eval: 3.319, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 3.385, Loss_eval: 3.336, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 2.985, Loss_eval: 3.359, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 3.446, Loss_eval: 3.381, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 3.263, Loss_eval: 3.417, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 3.049, Loss_eval: 3.359, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 3.208, Loss_eval: 3.360, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 3.052, Loss_eval: 3.385, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 2.928, Loss_eval: 3.423, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 3.275, Loss_eval: 3.368, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 3.332, Loss_eval: 3.366, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 3.280, Loss_eval: 3.387, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 3.165, Loss_eval: 3.379, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 3.410, Loss_eval: 3.365, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 3.215, Loss_eval: 3.372, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 3.211, Loss_eval: 3.415, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 3.140, Loss_eval: 3.399, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 3.408, Loss_eval: 3.392, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 3.206, Loss_eval: 3.356, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 3.075, Loss_eval: 3.387, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 2.971, Loss_eval: 3.398, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 3.214, Loss_eval: 3.368, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 3.517, Loss_eval: 3.385, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 3.494, Loss_eval: 3.402, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 3.366, Loss_eval: 3.353, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 3.316, Loss_eval: 3.345, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 3.277, Loss_eval: 3.388, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 3.354, Loss_eval: 3.344, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 3.287, Loss_eval: 3.356, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 3.107, Loss_eval: 3.354, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 3.353, Loss_eval: 3.394, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 3.502, Loss_eval: 3.364, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 3.371, Loss_eval: 3.358, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 3.338, Loss_eval: 3.358, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 3.244, Loss_eval: 3.353, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 2.935, Loss_eval: 3.364, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 3.199, Loss_eval: 3.339, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 3.268, Loss_eval: 3.363, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 3.370, Loss_eval: 3.334, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 3.357, Loss_eval: 3.334, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 3.321, Loss_eval: 3.386, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 3.013, Loss_eval: 3.365, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 3.351, Loss_eval: 3.341, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 3.205, Loss_eval: 3.370, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 3.091, Loss_eval: 3.362, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 3.376, Loss_eval: 3.380, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 3.340, Loss_eval: 3.292, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 3.103, Loss_eval: 3.379, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 3.232, Loss_eval: 3.316, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 3.025, Loss_eval: 3.390, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 3.368, Loss_eval: 3.350, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 3.163, Loss_eval: 3.422, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 3.294, Loss_eval: 3.354, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 3.543, Loss_eval: 3.383, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 2.729, Loss_eval: 3.376, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 2.990, Loss_eval: 3.315, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.452, Loss_eval: 3.369, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 3.609, Loss_eval: 3.381, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.107, Loss_eval: 3.340, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 3.247, Loss_eval: 3.332, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 3.461, Loss_eval: 3.419, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 3.368, Loss_eval: 3.364, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 3.283, Loss_eval: 3.370, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 2.839, Loss_eval: 3.323, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 3.328, Loss_eval: 3.314, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 3.143, Loss_eval: 3.338, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 3.338, Loss_eval: 3.397, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.019, Loss_eval: 3.338, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.151, Loss_eval: 3.354, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 3.302, Loss_eval: 3.337, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 3.425, Loss_eval: 3.377, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 3.118, Loss_eval: 3.335, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 3.585, Loss_eval: 3.332, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 3.447, Loss_eval: 3.386, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 3.396, Loss_eval: 3.336, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 3.263, Loss_eval: 3.329, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 3.069, Loss_eval: 3.321, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 3.443, Loss_eval: 3.316, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 3.077, Loss_eval: 3.424, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 2.968, Loss_eval: 3.337, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 3.846, Loss_eval: 3.337, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 3.208, Loss_eval: 3.361, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 3.308, Loss_eval: 3.353, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 3.086, Loss_eval: 3.363, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 3.404, Loss_eval: 3.333, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 3.337, Loss_eval: 3.321, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 2.931, Loss_eval: 3.341, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 3.165, Loss_eval: 3.385, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 3.302, Loss_eval: 3.363, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 3.298, Loss_eval: 3.344, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 2.998, Loss_eval: 3.358, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.246, Loss_eval: 3.335, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 3.360, Loss_eval: 3.324, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 3.072, Loss_eval: 3.382, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 3.212, Loss_eval: 3.307, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 3.464, Loss_eval: 3.380, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "filename = \"../../models/checkpoint_transformer_no_regularization_3epoch.pth\"\n",
    "\n",
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
