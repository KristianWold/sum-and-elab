{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformer_kristianwold.tokenizer import TokenizerBPE\n",
    "from transformer_kristianwold.utils import saver, loader\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e726f5",
   "metadata": {},
   "source": [
    "## Load cleaned corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1af1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../corpus/cnn_dailymail_highlight_train_cleaned.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m highlight_train_list = loader(\u001b[33m\"\u001b[39m\u001b[33m../corpus/cnn_dailymail_highlight_train_cleaned.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m article_train_list = loader(\u001b[33m\"\u001b[39m\u001b[33m../corpus/cnn_dailymail_article_train_cleaned.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m highlight_test_list = loader(\u001b[33m\"\u001b[39m\u001b[33m../corpus/cnn_dailymail_highlight_test_cleaned.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/sum_and_elab/src/transformer_kristianwold/utils.py:9\u001b[39m, in \u001b[36mloader\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloader\u001b[39m(filename):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pkl.load(f)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../corpus/cnn_dailymail_highlight_train_cleaned.pkl'"
     ]
    }
   ],
   "source": [
    "highlight_train_list = loader(\"../../corpus/cnn_dailymail_highlight_train_cleaned.pkl\")\n",
    "article_train_list = loader(\"../../corpus/cnn_dailymail_article_train_cleaned.pkl\")\n",
    "\n",
    "highlight_test_list = loader(\"../../corpus/cnn_dailymail_highlight_test_cleaned.pkl\")\n",
    "article_test_list = loader(\"../../corpus/cnn_dailymail_article_test_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = highlight_train_list + article_train_list + highlight_test_list + article_test_list\n",
    "print(f\"Total number of characters in the corpus: {len(\"\".join(corpus))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8a504",
   "metadata": {},
   "source": [
    "## Run Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70993e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerBPE(corpus=corpus, \n",
    "                         num_merges=24000,  # do 24k merges, resulting in ~24k vocabulary\n",
    "                         ratio=0.1,         # perform BPE on random 10% subset of words in corpus for efficiency\n",
    "                         verbose=True       # print merge details\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special tokens\n",
    "tokenizer.add_special_tokens([\"<s>\",  # start\n",
    "                              \"</s>\", # end\n",
    "                              \"<h>\",  # highlight\n",
    "                              \"<b>\"]) # bodytext                          \n",
    "\n",
    "saver(\"../tokenizers/cnn_tokenizer.pkl\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505c36f",
   "metadata": {},
   "source": [
    "## Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7092cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"../tokenizers/cnn_tokenizer3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909f1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(corpus_list):\n",
    "    corpus_list_new = []\n",
    "    for entry in corpus_list:\n",
    "        highlight, article = entry\n",
    "        new_entry = f\"<s><h>{highlight}<b>{article}</s>\"\n",
    "        corpus_list_new.append(new_entry)\n",
    "\n",
    "    return \"\".join(corpus_list_new)\n",
    "\n",
    "def add_special_tokens_HLlast(corpus_list):\n",
    "    corpus_list_new = []\n",
    "    for entry in corpus_list:\n",
    "        highlight, article = entry\n",
    "        new_entry = f\"<s><b>{article}<h>{highlight}</s>\"\n",
    "        corpus_list_new.append(new_entry)\n",
    "\n",
    "    return \"\".join(corpus_list_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee697ae0",
   "metadata": {},
   "source": [
    "## Tokenize Corpus\n",
    "\n",
    "### Highlight First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ee5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = add_special_tokens(list(zip(highlight_train_list, article_train_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize in four chunks to avoid memory problems\n",
    "\n",
    "corpus_train_tokens = tokenizer.encode(corpus_train[:length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens1.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc265b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//4:length//2], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens2.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//2:3*length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens3.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[3*length//4:], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens4.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results\n",
    "\n",
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens4.pkl\"))\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "saver(\"../corpus/cnn_dailymail_highlight_first_train.pkl\", corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563cb3b5",
   "metadata": {},
   "source": [
    "### Highlight Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0408d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = add_special_tokens_HLlast(list(zip(highlight_train_list, article_train_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize in four chunks to avoid memory problems\n",
    "\n",
    "corpus_train_tokens = tokenizer.encode(corpus_train[:length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens1.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//4:length//2], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens2.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3628a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//2:3*length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens3.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[3*length//4:], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens4.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results\n",
    "\n",
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens4.pkl\"))\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "saver(\"../corpus/cnn_dailymail_highlight_last_train.pkl\", corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c91cb",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ecf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = add_special_tokens(list(zip(highlight_test_list, article_test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = tokenizer.encode(corpus_test, verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_highlight_first_test.pkl\", torch.tensor(corpus_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54bca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = add_special_tokens_HLlast(list(zip(highlight_test_list, article_test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = tokenizer.encode(corpus_test, verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_highlight_last_test.pkl\", torch.tensor(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
