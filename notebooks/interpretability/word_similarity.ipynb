{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_kristianwold.transformer import Transformer, Inference\n",
    "from transformer_kristianwold.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from transformer_kristianwold.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_kristianwold.analysis import EmbeddingClustering\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b5a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"../../tokenizers/cnn_tokenizer3.pkl\")\n",
    "model = None\n",
    "def load_model(filename=None):\n",
    "    embed_dim = 64*18\n",
    "    ff_dim = 4*embed_dim\n",
    "    heads = 18\n",
    "    tf_blocks = 18\n",
    "\n",
    "    model = Transformer(\n",
    "        embed_dim=embed_dim,\n",
    "        ff_dim=ff_dim,\n",
    "        heads=heads,\n",
    "        tf_blocks=tf_blocks,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_seq_len=1024,\n",
    "        dropout=0.1,\n",
    "        start_token_id=tokenizer.token_to_idx[\"<s>\"],\n",
    "        use_weight_tying=True\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer_grouped_parameters = group_decay_parameters(\n",
    "        model,\n",
    "        weight_decay=0.1,\n",
    "        no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "        )\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "    loss_train_list = []\n",
    "    loss_test_list = []\n",
    "\n",
    "    num_epochs      = 1\n",
    "    steps_per_epoch = 1\n",
    "    warmup_steps    = 1000\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        return 1.0\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    if filename is not None:\n",
    "        [model, \n",
    "        _, \n",
    "        _, \n",
    "        loss_train_list, \n",
    "        loss_test_list] = load_checkpoint(filename, \n",
    "                                        model, \n",
    "                                        optimizer, \n",
    "                                        scheduler, \n",
    "                                        loss_train_list, \n",
    "                                        loss_test_list)\n",
    "\n",
    "    return model, loss_train_list, loss_test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7d41a",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_train_list, loss_test_list = load_model(\"../../models/checkpoint_transformer_5epoch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c06030",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"obama\"\n",
    "token = tokenizer.encode(text)\n",
    "print(\"Word:\", text)\n",
    "print(\"Tokenized:\", token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
