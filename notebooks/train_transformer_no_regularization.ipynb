{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_kristianwold.transformer import Transformer\n",
    "from transformer_kristianwold.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from transformer_kristianwold.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token id: 24070\n",
      "Vocab size: 24074\n"
     ]
    }
   ],
   "source": [
    "tokenizer = loader(\"../tokenizers/cnn_tokenizer3.pkl\")\n",
    "\n",
    "start_token_id=tokenizer.token_to_idx[\"<s>\"]\n",
    "vocab_size=tokenizer.vocab_size\n",
    "\n",
    "print(\"Start token id:\", start_token_id)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train1 = loader(\"../corpus/cnn_dailymail_highlight_first_train.pkl\")\n",
    "corpus_train2 = loader(\"../corpus/cnn_dailymail_highlight_last_train.pkl\")\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2), dim=0)\n",
    "\n",
    "corpus_test1 = loader(\"../corpus/cnn_dailymail_highlight_first_test.pkl\")\n",
    "corpus_test2 = loader(\"../corpus/cnn_dailymail_highlight_last_test.pkl\")\n",
    "corpus_test = torch.cat((corpus_test1, corpus_test2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 315778058\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.,   # no dropout\n",
    "    start_token_id=start_token_id,\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0., #no weight decay\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "filename = \"checkpoint_transformer_no_regularization_1epoch.pth\"\n",
    "\n",
    "print(\"Number of parameters:\", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ee96f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cafe8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint(model, \n",
    "#                 optimizer, \n",
    "#                 scheduler,\n",
    "#                 loss_train_list,\n",
    "#                 loss_test_list, \n",
    "#                 filename=\"models/checkpoint_transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7c59",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5961af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[model, \n",
    "#optimizer, \n",
    "#scheduler, \n",
    "#loss_train_list, \n",
    "#loss_test_list] = load_checkpoint(\"../models/checkpoint_transformer_3epoch.pth\", \n",
    "#                                  model, \n",
    "#                                  optimizer, \n",
    "#                                  scheduler, \n",
    "#                                  loss_train_list, \n",
    "#                                  loss_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4236bf9a98464c9714ed353cb4b809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 10.028, Loss_eval: 10.034, Learning Rate: 6.000000e-07\n",
      "Step 1000, Loss: 9.842, Loss_eval: 9.838, Learning Rate: 1.250000e-06\n",
      "Step 1500, Loss: 9.666, Loss_eval: 9.648, Learning Rate: 1.850000e-06\n",
      "Step 2000, Loss: 9.444, Loss_eval: 9.482, Learning Rate: 2.500000e-06\n",
      "Step 2500, Loss: 9.373, Loss_eval: 9.383, Learning Rate: 3.100000e-06\n",
      "Step 3000, Loss: 9.250, Loss_eval: 9.305, Learning Rate: 3.750000e-06\n",
      "Step 3500, Loss: 9.233, Loss_eval: 9.230, Learning Rate: 4.350000e-06\n",
      "Step 4000, Loss: 9.156, Loss_eval: 9.163, Learning Rate: 5.000000e-06\n",
      "Step 4500, Loss: 9.034, Loss_eval: 9.072, Learning Rate: 5.600000e-06\n",
      "Step 5000, Loss: 8.974, Loss_eval: 8.964, Learning Rate: 6.250000e-06\n",
      "Step 5500, Loss: 8.841, Loss_eval: 8.866, Learning Rate: 6.850000e-06\n",
      "Step 6000, Loss: 8.749, Loss_eval: 8.748, Learning Rate: 7.500000e-06\n",
      "Step 6500, Loss: 8.605, Loss_eval: 8.636, Learning Rate: 8.100000e-06\n",
      "Step 7000, Loss: 8.485, Loss_eval: 8.504, Learning Rate: 8.750000e-06\n",
      "Step 7500, Loss: 8.299, Loss_eval: 8.370, Learning Rate: 9.350000e-06\n",
      "Step 8000, Loss: 8.247, Loss_eval: 8.221, Learning Rate: 1.000000e-05\n",
      "Step 8500, Loss: 8.049, Loss_eval: 8.121, Learning Rate: 1.060000e-05\n",
      "Step 9000, Loss: 7.912, Loss_eval: 7.976, Learning Rate: 1.125000e-05\n",
      "Step 9500, Loss: 7.840, Loss_eval: 7.854, Learning Rate: 1.185000e-05\n",
      "Step 10000, Loss: 7.669, Loss_eval: 7.744, Learning Rate: 1.250000e-05\n",
      "Step 10500, Loss: 7.742, Loss_eval: 7.648, Learning Rate: 1.310000e-05\n",
      "Step 11000, Loss: 7.429, Loss_eval: 7.540, Learning Rate: 1.375000e-05\n",
      "Step 11500, Loss: 7.330, Loss_eval: 7.489, Learning Rate: 1.435000e-05\n",
      "Step 12000, Loss: 7.120, Loss_eval: 7.392, Learning Rate: 1.500000e-05\n",
      "Step 12500, Loss: 7.192, Loss_eval: 7.334, Learning Rate: 1.560000e-05\n",
      "Step 13000, Loss: 7.073, Loss_eval: 7.275, Learning Rate: 1.625000e-05\n",
      "Step 13500, Loss: 6.941, Loss_eval: 7.184, Learning Rate: 1.685000e-05\n",
      "Step 14000, Loss: 7.126, Loss_eval: 7.134, Learning Rate: 1.750000e-05\n",
      "Step 14500, Loss: 7.110, Loss_eval: 7.102, Learning Rate: 1.810000e-05\n",
      "Step 15000, Loss: 6.833, Loss_eval: 7.034, Learning Rate: 1.875000e-05\n",
      "Step 15500, Loss: 6.802, Loss_eval: 6.975, Learning Rate: 1.935000e-05\n",
      "Step 16000, Loss: 6.826, Loss_eval: 6.945, Learning Rate: 2.000000e-05\n",
      "Step 16500, Loss: 6.768, Loss_eval: 6.863, Learning Rate: 2.060000e-05\n",
      "Step 17000, Loss: 6.662, Loss_eval: 6.804, Learning Rate: 2.125000e-05\n",
      "Step 17500, Loss: 6.635, Loss_eval: 6.769, Learning Rate: 2.185000e-05\n",
      "Step 18000, Loss: 6.524, Loss_eval: 6.741, Learning Rate: 2.250000e-05\n",
      "Step 18500, Loss: 6.522, Loss_eval: 6.682, Learning Rate: 2.310000e-05\n",
      "Step 19000, Loss: 6.651, Loss_eval: 6.671, Learning Rate: 2.375000e-05\n",
      "Step 19500, Loss: 6.532, Loss_eval: 6.605, Learning Rate: 2.435000e-05\n",
      "Step 20000, Loss: 6.640, Loss_eval: 6.576, Learning Rate: 2.500000e-05\n",
      "Step 20500, Loss: 6.517, Loss_eval: 6.595, Learning Rate: 2.560000e-05\n",
      "Step 21000, Loss: 6.560, Loss_eval: 6.528, Learning Rate: 2.625000e-05\n",
      "Step 21500, Loss: 6.405, Loss_eval: 6.506, Learning Rate: 2.685000e-05\n",
      "Step 22000, Loss: 6.322, Loss_eval: 6.442, Learning Rate: 2.750000e-05\n",
      "Step 22500, Loss: 6.457, Loss_eval: 6.429, Learning Rate: 2.810000e-05\n",
      "Step 23000, Loss: 6.224, Loss_eval: 6.423, Learning Rate: 2.875000e-05\n",
      "Step 23500, Loss: 6.334, Loss_eval: 6.371, Learning Rate: 2.935000e-05\n",
      "Step 24000, Loss: 6.316, Loss_eval: 6.369, Learning Rate: 3.000000e-05\n",
      "Step 24500, Loss: 6.287, Loss_eval: 6.347, Learning Rate: 3.060000e-05\n",
      "Step 25000, Loss: 6.245, Loss_eval: 6.333, Learning Rate: 3.125000e-05\n",
      "Step 25500, Loss: 6.139, Loss_eval: 6.296, Learning Rate: 3.185000e-05\n",
      "Step 26000, Loss: 6.385, Loss_eval: 6.250, Learning Rate: 3.250000e-05\n",
      "Step 26500, Loss: 6.191, Loss_eval: 6.250, Learning Rate: 3.310000e-05\n",
      "Step 27000, Loss: 6.155, Loss_eval: 6.254, Learning Rate: 3.375000e-05\n",
      "Step 27500, Loss: 6.424, Loss_eval: 6.238, Learning Rate: 3.435000e-05\n",
      "Step 28000, Loss: 6.127, Loss_eval: 6.216, Learning Rate: 3.500000e-05\n",
      "Step 28500, Loss: 6.277, Loss_eval: 6.216, Learning Rate: 3.560000e-05\n",
      "Step 29000, Loss: 5.895, Loss_eval: 6.159, Learning Rate: 3.625000e-05\n",
      "Step 29500, Loss: 6.139, Loss_eval: 6.181, Learning Rate: 3.685000e-05\n",
      "Step 30000, Loss: 5.901, Loss_eval: 6.163, Learning Rate: 3.750000e-05\n",
      "Step 30500, Loss: 5.963, Loss_eval: 6.138, Learning Rate: 3.810000e-05\n",
      "Step 31000, Loss: 6.090, Loss_eval: 6.082, Learning Rate: 3.875000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e12e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"checkpoint_transformer_no_regularization_2epoch.pth\"\n",
    "\n",
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"checkpoint_transformer_no_regularization_3epoch.pth\"\n",
    "\n",
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
