{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_kristianwold.transformer import Transformer\n",
    "from transformer_kristianwold.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from transformer_kristianwold.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token id: 24070\n",
      "Vocab size: 24074\n"
     ]
    }
   ],
   "source": [
    "tokenizer = loader(\"../tokenizers/cnn_tokenizer3.pkl\")\n",
    "\n",
    "start_token_id=tokenizer.token_to_idx[\"<s>\"]\n",
    "vocab_size=tokenizer.vocab_size\n",
    "\n",
    "print(\"Start token id:\", start_token_id)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train1 = loader(\"../corpus/cnn_dailymail_highlight_first_train.pkl\")\n",
    "corpus_train2 = loader(\"../corpus/cnn_dailymail_highlight_last_train.pkl\")\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2), dim=0)\n",
    "\n",
    "corpus_test1 = loader(\"../corpus/cnn_dailymail_highlight_first_test.pkl\")\n",
    "corpus_test2 = loader(\"../corpus/cnn_dailymail_highlight_last_test.pkl\")\n",
    "corpus_test = torch.cat((corpus_test1, corpus_test2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 315778058\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=start_token_id,\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "filename = \"checkpoint_transformer_4epoch.pth\"\n",
    "\n",
    "print(\"Number of parameters:\", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "loss_train_list = []\n",
    "loss_test_list = []\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ee96f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafe8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint(model, \n",
    "#                 optimizer, \n",
    "#                 scheduler,\n",
    "#                 loss_train_list,\n",
    "#                 loss_test_list, \n",
    "#                 filename=\"models/checkpoint_transformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7c59",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model, \n",
    "optimizer, \n",
    "scheduler, \n",
    "loss_train_list, \n",
    "loss_test_list] = load_checkpoint(\"../models/checkpoint_transformer_3epoch.pth\", \n",
    "                                  model, \n",
    "                                  optimizer, \n",
    "                                  scheduler, \n",
    "                                  loss_train_list, \n",
    "                                  loss_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c114ea24c140d0901d5cd73faa4495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 3.990, Loss_eval: 4.073, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 4.085, Loss_eval: 4.062, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 4.084, Loss_eval: 4.077, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 4.017, Loss_eval: 4.039, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 3.913, Loss_eval: 4.016, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 3.806, Loss_eval: 4.025, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 3.768, Loss_eval: 4.045, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 4.066, Loss_eval: 4.060, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 4.065, Loss_eval: 4.036, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 4.095, Loss_eval: 4.037, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 4.035, Loss_eval: 4.035, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 4.084, Loss_eval: 4.041, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 4.008, Loss_eval: 4.074, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 3.957, Loss_eval: 4.056, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.914, Loss_eval: 3.993, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 4.325, Loss_eval: 4.053, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 4.000, Loss_eval: 4.017, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 4.009, Loss_eval: 4.038, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 4.011, Loss_eval: 3.998, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 4.000, Loss_eval: 3.994, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 4.103, Loss_eval: 4.012, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 4.010, Loss_eval: 3.976, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.967, Loss_eval: 4.044, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.839, Loss_eval: 4.034, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 4.052, Loss_eval: 4.025, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 4.002, Loss_eval: 4.007, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.870, Loss_eval: 3.954, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 3.834, Loss_eval: 4.018, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 4.290, Loss_eval: 4.014, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 4.170, Loss_eval: 3.988, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 4.106, Loss_eval: 4.025, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.976, Loss_eval: 4.037, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.981, Loss_eval: 3.947, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 3.736, Loss_eval: 3.957, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 3.791, Loss_eval: 3.944, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.908, Loss_eval: 3.998, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 3.872, Loss_eval: 3.984, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 4.051, Loss_eval: 4.016, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 4.182, Loss_eval: 3.958, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 4.157, Loss_eval: 3.963, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 3.814, Loss_eval: 3.952, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 3.742, Loss_eval: 3.987, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 3.932, Loss_eval: 3.999, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.912, Loss_eval: 3.967, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 4.144, Loss_eval: 3.959, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 3.762, Loss_eval: 4.007, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.921, Loss_eval: 3.964, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 3.993, Loss_eval: 4.000, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 4.007, Loss_eval: 3.977, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 4.090, Loss_eval: 3.948, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 3.857, Loss_eval: 3.974, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.994, Loss_eval: 3.965, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 4.016, Loss_eval: 3.903, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 3.729, Loss_eval: 3.935, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 4.217, Loss_eval: 3.940, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.854, Loss_eval: 3.935, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 4.037, Loss_eval: 3.968, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 3.835, Loss_eval: 3.953, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.975, Loss_eval: 3.968, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 4.055, Loss_eval: 3.989, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.841, Loss_eval: 3.999, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 4.137, Loss_eval: 3.931, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.958, Loss_eval: 3.934, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 4.070, Loss_eval: 3.931, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.760, Loss_eval: 3.944, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 3.683, Loss_eval: 3.924, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.990, Loss_eval: 3.928, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.760, Loss_eval: 3.946, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.957, Loss_eval: 3.950, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.960, Loss_eval: 3.923, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 4.175, Loss_eval: 3.891, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 3.601, Loss_eval: 3.852, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.906, Loss_eval: 3.910, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.828, Loss_eval: 3.933, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 4.003, Loss_eval: 3.884, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.787, Loss_eval: 3.885, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 4.313, Loss_eval: 3.929, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.788, Loss_eval: 3.937, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 4.181, Loss_eval: 3.867, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 4.117, Loss_eval: 3.871, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.824, Loss_eval: 3.829, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 3.647, Loss_eval: 3.947, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 3.721, Loss_eval: 3.876, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.981, Loss_eval: 3.898, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.926, Loss_eval: 3.903, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.816, Loss_eval: 3.867, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 3.986, Loss_eval: 3.920, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.743, Loss_eval: 3.872, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.905, Loss_eval: 3.989, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.779, Loss_eval: 3.857, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 3.744, Loss_eval: 3.841, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 3.819, Loss_eval: 3.918, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.932, Loss_eval: 3.910, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 4.032, Loss_eval: 3.891, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 3.509, Loss_eval: 3.854, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.823, Loss_eval: 3.851, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 3.468, Loss_eval: 3.858, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 4.117, Loss_eval: 3.895, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 3.773, Loss_eval: 3.863, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.798, Loss_eval: 3.880, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 3.992, Loss_eval: 3.912, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.887, Loss_eval: 3.878, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 3.914, Loss_eval: 3.885, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.755, Loss_eval: 3.856, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 4.046, Loss_eval: 3.899, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.988, Loss_eval: 3.827, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.878, Loss_eval: 3.857, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 3.753, Loss_eval: 3.833, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 3.916, Loss_eval: 3.874, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 4.088, Loss_eval: 3.879, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.976, Loss_eval: 3.850, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 3.476, Loss_eval: 3.876, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 4.280, Loss_eval: 3.818, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.700, Loss_eval: 3.874, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 3.934, Loss_eval: 3.858, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.791, Loss_eval: 3.856, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 3.783, Loss_eval: 3.822, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 3.945, Loss_eval: 3.838, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 3.664, Loss_eval: 3.922, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 3.820, Loss_eval: 3.799, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 3.721, Loss_eval: 3.867, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 3.529, Loss_eval: 3.797, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 3.662, Loss_eval: 3.874, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.754, Loss_eval: 3.851, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 3.700, Loss_eval: 3.822, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.894, Loss_eval: 3.805, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 4.039, Loss_eval: 3.823, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 3.749, Loss_eval: 3.817, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 3.637, Loss_eval: 3.818, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 3.548, Loss_eval: 3.777, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 3.533, Loss_eval: 3.856, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 3.648, Loss_eval: 3.845, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 3.539, Loss_eval: 3.775, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 4.095, Loss_eval: 3.845, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 3.480, Loss_eval: 3.921, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.786, Loss_eval: 3.821, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 3.702, Loss_eval: 3.868, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.725, Loss_eval: 3.837, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 3.566, Loss_eval: 3.847, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 3.641, Loss_eval: 3.818, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.890, Loss_eval: 3.844, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.851, Loss_eval: 3.780, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 3.408, Loss_eval: 3.799, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 3.696, Loss_eval: 3.901, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.715, Loss_eval: 3.799, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.962, Loss_eval: 3.813, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.741, Loss_eval: 3.762, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.813, Loss_eval: 3.777, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.800, Loss_eval: 3.785, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 3.494, Loss_eval: 3.772, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 3.575, Loss_eval: 3.803, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 3.358, Loss_eval: 3.751, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 3.587, Loss_eval: 3.831, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 3.846, Loss_eval: 3.834, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 3.586, Loss_eval: 3.815, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 3.903, Loss_eval: 3.795, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 3.639, Loss_eval: 3.832, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 3.520, Loss_eval: 3.808, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 3.598, Loss_eval: 3.819, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 4.077, Loss_eval: 3.806, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 3.914, Loss_eval: 3.862, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 3.442, Loss_eval: 3.797, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 4.105, Loss_eval: 3.766, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 3.843, Loss_eval: 3.838, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 3.521, Loss_eval: 3.765, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 3.960, Loss_eval: 3.814, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 3.679, Loss_eval: 3.737, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 3.542, Loss_eval: 3.767, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 3.668, Loss_eval: 3.793, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 4.027, Loss_eval: 3.764, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 3.912, Loss_eval: 3.779, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 3.642, Loss_eval: 3.785, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 3.939, Loss_eval: 3.780, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 3.729, Loss_eval: 3.781, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 3.341, Loss_eval: 3.769, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 3.536, Loss_eval: 3.748, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 3.743, Loss_eval: 3.766, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 3.634, Loss_eval: 3.826, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 3.817, Loss_eval: 3.786, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 3.613, Loss_eval: 3.759, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 3.767, Loss_eval: 3.763, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 3.725, Loss_eval: 3.761, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 3.772, Loss_eval: 3.770, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 3.724, Loss_eval: 3.703, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 4.184, Loss_eval: 3.814, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 3.851, Loss_eval: 3.739, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 3.957, Loss_eval: 3.757, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 3.576, Loss_eval: 3.773, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 3.851, Loss_eval: 3.759, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 3.735, Loss_eval: 3.784, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 3.612, Loss_eval: 3.761, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 3.550, Loss_eval: 3.737, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 3.740, Loss_eval: 3.792, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 3.640, Loss_eval: 3.718, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 3.802, Loss_eval: 3.776, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 3.277, Loss_eval: 3.772, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 3.588, Loss_eval: 3.742, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 3.420, Loss_eval: 3.696, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 3.889, Loss_eval: 3.729, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 3.465, Loss_eval: 3.781, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 3.656, Loss_eval: 3.771, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 3.783, Loss_eval: 3.797, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 3.608, Loss_eval: 3.787, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 3.663, Loss_eval: 3.753, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 3.956, Loss_eval: 3.793, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 3.826, Loss_eval: 3.740, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 3.738, Loss_eval: 3.751, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 3.580, Loss_eval: 3.726, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 3.864, Loss_eval: 3.734, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 3.619, Loss_eval: 3.728, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 3.507, Loss_eval: 3.701, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 3.718, Loss_eval: 3.707, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 3.451, Loss_eval: 3.718, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 3.553, Loss_eval: 3.766, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 3.854, Loss_eval: 3.664, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 3.268, Loss_eval: 3.746, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 3.306, Loss_eval: 3.704, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 3.288, Loss_eval: 3.762, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 3.557, Loss_eval: 3.746, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 3.774, Loss_eval: 3.777, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 3.667, Loss_eval: 3.713, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 4.022, Loss_eval: 3.775, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 3.497, Loss_eval: 3.748, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 3.559, Loss_eval: 3.698, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 3.746, Loss_eval: 3.747, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 3.714, Loss_eval: 3.703, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 3.706, Loss_eval: 3.723, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 3.814, Loss_eval: 3.725, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 4.338, Loss_eval: 3.713, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 3.628, Loss_eval: 3.734, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 3.887, Loss_eval: 3.697, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 3.889, Loss_eval: 3.672, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 3.604, Loss_eval: 3.736, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 3.742, Loss_eval: 3.733, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 3.820, Loss_eval: 3.671, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 3.572, Loss_eval: 3.705, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 3.966, Loss_eval: 3.698, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 3.789, Loss_eval: 3.704, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 3.735, Loss_eval: 3.696, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 3.522, Loss_eval: 3.699, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 3.859, Loss_eval: 3.744, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 3.592, Loss_eval: 3.726, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 3.858, Loss_eval: 3.723, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 3.738, Loss_eval: 3.702, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 3.795, Loss_eval: 3.728, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 3.497, Loss_eval: 3.692, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 3.588, Loss_eval: 3.689, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 4.061, Loss_eval: 3.684, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 3.456, Loss_eval: 3.718, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 3.592, Loss_eval: 3.680, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 3.558, Loss_eval: 3.695, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 3.829, Loss_eval: 3.724, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 3.532, Loss_eval: 3.666, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 3.541, Loss_eval: 3.695, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 3.417, Loss_eval: 3.698, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 3.425, Loss_eval: 3.659, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 3.533, Loss_eval: 3.691, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 3.622, Loss_eval: 3.668, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 3.654, Loss_eval: 3.679, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 3.878, Loss_eval: 3.675, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 3.527, Loss_eval: 3.692, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 3.537, Loss_eval: 3.677, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 3.548, Loss_eval: 3.666, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 3.734, Loss_eval: 3.677, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 3.689, Loss_eval: 3.669, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 3.143, Loss_eval: 3.647, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 3.942, Loss_eval: 3.685, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 3.646, Loss_eval: 3.738, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 3.847, Loss_eval: 3.691, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 3.693, Loss_eval: 3.665, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 3.604, Loss_eval: 3.698, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 3.480, Loss_eval: 3.640, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 4.009, Loss_eval: 3.682, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 3.766, Loss_eval: 3.698, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 3.535, Loss_eval: 3.689, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 3.374, Loss_eval: 3.695, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 3.865, Loss_eval: 3.663, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 3.542, Loss_eval: 3.679, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 3.938, Loss_eval: 3.645, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 3.733, Loss_eval: 3.611, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 3.565, Loss_eval: 3.648, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 3.582, Loss_eval: 3.645, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 3.739, Loss_eval: 3.655, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 3.742, Loss_eval: 3.675, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 3.554, Loss_eval: 3.637, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 3.444, Loss_eval: 3.639, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 3.673, Loss_eval: 3.710, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 3.744, Loss_eval: 3.654, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 3.397, Loss_eval: 3.595, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 3.596, Loss_eval: 3.679, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 3.737, Loss_eval: 3.654, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 3.933, Loss_eval: 3.687, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 3.655, Loss_eval: 3.674, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 3.473, Loss_eval: 3.678, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 3.584, Loss_eval: 3.636, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 3.814, Loss_eval: 3.694, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 3.628, Loss_eval: 3.681, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 3.990, Loss_eval: 3.667, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 3.345, Loss_eval: 3.632, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 3.612, Loss_eval: 3.671, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 3.399, Loss_eval: 3.638, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 4.059, Loss_eval: 3.650, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 3.821, Loss_eval: 3.700, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 3.582, Loss_eval: 3.637, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 3.546, Loss_eval: 3.618, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 3.956, Loss_eval: 3.648, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 3.506, Loss_eval: 3.646, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 3.624, Loss_eval: 3.718, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 3.600, Loss_eval: 3.576, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 3.894, Loss_eval: 3.702, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.536, Loss_eval: 3.693, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 3.778, Loss_eval: 3.692, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.532, Loss_eval: 3.621, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 3.905, Loss_eval: 3.649, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 3.776, Loss_eval: 3.630, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 3.415, Loss_eval: 3.619, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 3.374, Loss_eval: 3.642, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 3.576, Loss_eval: 3.636, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 3.697, Loss_eval: 3.605, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 3.567, Loss_eval: 3.647, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 3.421, Loss_eval: 3.609, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.558, Loss_eval: 3.621, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.554, Loss_eval: 3.687, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 3.610, Loss_eval: 3.666, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 3.686, Loss_eval: 3.674, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 3.611, Loss_eval: 3.612, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 3.663, Loss_eval: 3.581, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 3.413, Loss_eval: 3.629, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 3.546, Loss_eval: 3.621, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 3.545, Loss_eval: 3.608, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 3.375, Loss_eval: 3.634, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 3.462, Loss_eval: 3.608, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 3.530, Loss_eval: 3.663, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 3.444, Loss_eval: 3.658, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 3.938, Loss_eval: 3.609, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 3.740, Loss_eval: 3.648, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 3.717, Loss_eval: 3.635, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 3.529, Loss_eval: 3.625, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 3.721, Loss_eval: 3.666, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 3.373, Loss_eval: 3.662, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 3.351, Loss_eval: 3.627, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 3.799, Loss_eval: 3.694, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 3.345, Loss_eval: 3.610, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 3.488, Loss_eval: 3.643, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 3.867, Loss_eval: 3.596, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.533, Loss_eval: 3.548, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 3.620, Loss_eval: 3.629, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 3.314, Loss_eval: 3.580, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 3.708, Loss_eval: 3.613, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 3.457, Loss_eval: 3.569, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        \n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_test = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() \n",
    "                                     for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.3f}, Loss_eval: {loss_test:<.3f}, Learning Rate: {lr:3e}\")\n",
    "            model.train()\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_test_list.append(loss_test)\n",
    "\n",
    "            \n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            loss_train_list,\n",
    "                            loss_test_list, \n",
    "                            filename=filename)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss_train_list,\n",
    "                    loss_test_list, \n",
    "                    filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
