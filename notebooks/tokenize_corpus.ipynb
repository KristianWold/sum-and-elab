{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle as pkl\n",
    "\n",
    "from transformer_kristianwold.tokenizer import TokenizerBPE\n",
    "from transformer_kristianwold.utils import saver, loader\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e726f5",
   "metadata": {},
   "source": [
    "## Load cleaned corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a1af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_train_list = loader(\"../corpus/cnn_dailymail_highlight_train_cleaned.pkl\")\n",
    "article_train_list = loader(\"../corpus/cnn_dailymail_article_train_cleaned.pkl\")\n",
    "\n",
    "highlight_test_list = loader(\"../corpus/cnn_dailymail_highlight_test_cleaned.pkl\")\n",
    "article_test_list = loader(\"../corpus/cnn_dailymail_article_test_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9d614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the corpus: 1288375458\n"
     ]
    }
   ],
   "source": [
    "corpus = highlight_train_list + article_train_list + highlight_test_list + article_test_list\n",
    "print(f\"Total number of characters in the corpus: {len(\"\".join(corpus))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8a504",
   "metadata": {},
   "source": [
    "## Run Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70993e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create character tokenizer\n",
      "Split corpus into words\n",
      "Char tokenize corpus\n",
      "Pre-merge corpus\n",
      "Merging started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd97876c3944860ab1c71a5a686416f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging tokens:   + t -> 70 with count 3101955\n",
      "Merging tokens:   + a -> 71 with count 2468422\n",
      "Merging tokens: h + e -> 72 with count 2334041\n",
      "Merging tokens: i + n -> 73 with count 2005509\n",
      "Merging tokens:   + s -> 74 with count 1608020\n",
      "Merging tokens:  t + he -> 75 with count 1474223\n",
      "Merging tokens: r + e -> 76 with count 1374470\n",
      "Merging tokens:   + w -> 77 with count 1323461\n",
      "Merging tokens: o + n -> 78 with count 1242843\n",
      "Merging tokens: e + r -> 79 with count 1225920\n",
      "Merging complete\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerBPE(corpus=corpus, \n",
    "                         num_merges=10,  # do 24k merges, resulting in ~24k tokens\n",
    "                         ratio=0.1,         # perform BPE on random 10% subset of words in corpus for efficiency\n",
    "                         verbose=True       # print merge details\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25c49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = pkl.load(open(\"stuff.pkl\", \"rb\"))\n",
    "[tokenizer.tokenizer_char.token_to_idx, \n",
    "tokenizer.token_to_idx, \n",
    "tokenizer.idx_to_token, \n",
    "tokenizer.pre_merge_list,\n",
    "tokenizer.merge_list] = stuff\n",
    "\n",
    "saver(\"../tokenizers/cnn_tokenizer3.pkl\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special tokens\n",
    "tokenizer.add_special_tokens([\"<s>\",  # start\n",
    "                              \"</s>\", # end\n",
    "                              \"<h>\",  # highlight\n",
    "                              \"<b>\"]) # bodytext                          \n",
    "\n",
    "saver(\"../tokenizers/cnn_tokenizer.pkl\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505c36f",
   "metadata": {},
   "source": [
    "## Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7092cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"../tokenizers/cnn_tokenizer3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "909f1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(corpus_list):\n",
    "    corpus_list_new = []\n",
    "    for entry in tqdm(corpus_list, desc=\"Adding start and stop tokens\"):\n",
    "        highlight, article = entry\n",
    "        new_entry = f\"<s><h>{highlight}<b>{article}</s>\"\n",
    "        corpus_list_new.append(new_entry)\n",
    "\n",
    "    return \"\".join(corpus_list_new)\n",
    "\n",
    "def add_special_tokens_HLlast(corpus_list):\n",
    "    corpus_list_new = []\n",
    "    for entry in tqdm(corpus_list, desc=\"Adding start and stop tokens\"):\n",
    "        highlight, article = entry\n",
    "        new_entry = f\"<s><b>{article}<h>{highlight}</s>\"\n",
    "        corpus_list_new.append(new_entry)\n",
    "\n",
    "    return \"\".join(corpus_list_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee697ae0",
   "metadata": {},
   "source": [
    "## Tokenize Corpus\n",
    "\n",
    "### Highlight First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ee5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = add_special_tokens(list(zip(highlight_train_list, article_train_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize in four chunks to avoid memory problems\n",
    "\n",
    "corpus_train_tokens = tokenizer.encode(corpus_train[:length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens1.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc265b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//4:length//2], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens2.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//2:3*length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens3.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[3*length//4:], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_article_train_tokens4.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results\n",
    "\n",
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens4.pkl\"))\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "saver(\"../corpus/cnn_dailymail_highlight_first_train.pkl\", corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563cb3b5",
   "metadata": {},
   "source": [
    "### Highlight Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0408d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = add_special_tokens_HLlast(list(zip(highlight_train_list, article_train_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize in four chunks to avoid memory problems\n",
    "\n",
    "corpus_train_tokens = tokenizer.encode(corpus_train[:length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens1.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//4:length//2], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens2.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3628a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//2:3*length//4], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens3.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[3*length//4:], verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_HLlast_train_tokens4.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate results\n",
    "\n",
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens4.pkl\"))\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "saver(\"../corpus/cnn_dailymail_highlight_last_train.pkl\", corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c91cb",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ecf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = add_special_tokens(list(zip(highlight_test_list, article_test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = tokenizer.encode(corpus_test, verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_highlight_first_test.pkl\", torch.tensor(corpus_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e54bca63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc805b2c8b31418f8d031b2b3553ce56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding start and stop tokens:   0%|          | 0/11490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_test = add_special_tokens_HLlast(list(zip(highlight_test_list, article_test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124082fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972e024a214c49eeb8bfe8411275060d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_test = tokenizer.encode(corpus_test, verbose=True)\n",
    "saver(\"../corpus/cnn_dailymail_highlight_last_test.pkl\", torch.tensor(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
